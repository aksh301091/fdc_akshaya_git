{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e080b77-4fbe-4fa9-aae1-61a125d33f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fosforio import snowflake\n",
    "from fosforml import *\n",
    "from fosforml.constants import MLModelFlavours\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from joblib import dump, load\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "from time import sleep\n",
    "import configparser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from dateutil.easter import easter\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import curve_fit\n",
    "import holidays, itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b0e311-380d-4b89-8c32-a341ffc43c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "my_session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd407cf-43fe-4444-b9e2-bea2ad3c2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session.sql('''\n",
    "CREATE OR REPLACE PROCEDURE create_pricing_model(table_name STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.8'\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pandas', 'numpy', 'scipy')\n",
    "HANDLER = 'create_pricing_model'\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar, curve_fit, brentq\n",
    "import itertools\n",
    "\n",
    "# Main procedure to create pricing model\n",
    "def create_pricing_model(session, table_name=None):\n",
    "\n",
    "    # Define the revenue function based on price and demand curve\n",
    "    def revenue(price):\n",
    "        return price * demand_curve(price, a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "\n",
    "    # Define the demand curve as an exponential function\n",
    "    def demand_curve(x, a, b, c, d, max_demand):\n",
    "        demand = a * np.exp(-b * x) + c\n",
    "        demand = np.where(x <= max_demand, np.minimum(demand, max_demand), demand)\n",
    "        return demand + d\n",
    "\n",
    "    # Calculate price based on the number of rooms\n",
    "    def demand_to_price(num_rooms, a, b, c, d, max_demand):\n",
    "        def root_func(x):\n",
    "            return num_rooms - (a * np.exp(-b * x) + c)\n",
    "\n",
    "        try:\n",
    "            price = brentq(root_func, 0, 200)\n",
    "        except ValueError:\n",
    "            price_range = (0, 200)\n",
    "            price = np.random.uniform(*price_range)\n",
    "\n",
    "        return price\n",
    "\n",
    "    # Retrieve data from Snowflake table and transform it to a Pandas DataFrame\n",
    "    expanded_df = session.table([table_name]).to_pandas()\n",
    "    expanded_df.columns = map(lambda x: str(x).lower(), expanded_df.columns)\n",
    "\n",
    "    # Initialize an empty DataFrame for storing results\n",
    "    results = pd.DataFrame(columns=['month', 'hotel', 'room_limit', 'room_type', 'dow', 'optimal_rate', 'expected_rn', 'expected_rev', 'optimal_rate_lim_inv'])\n",
    "    \n",
    "    hotel_types = ['Resort Hotel', 'City Hotel']\n",
    "    room_types = ['A', 'D', 'E']\n",
    "\n",
    "    # Aggregate daily room numbers and merge with ADR frequency data\n",
    "    daily_rns = expanded_df.groupby(['arrival_date', 'dow', 'month', 'hotel', 'room_type']).agg({'room_limit': 'mean', 'total_rns': 'sum'}).reset_index()\n",
    "    daily_rns = daily_rns.groupby(['dow', 'month', 'hotel', 'room_type']).agg({'room_limit': 'mean', 'total_rns': ['sum', 'mean', 'median']}).reset_index()\n",
    "    daily_rns.columns = ['_'.join(col) for col in daily_rns.columns]\n",
    "    \n",
    "    adr_frequency = expanded_df.groupby(['dow', 'month', 'adr', 'hotel', 'room_type']).agg({'room_limit': 'mean', 'total_rns': 'sum'})\n",
    "    adr_frequency.reset_index(inplace=True)\n",
    "    \n",
    "    merged_df = pd.merge(adr_frequency, daily_rns, how='left', left_on=['dow', 'month', 'hotel', 'room_type'], right_on=['dow_', 'month_', 'hotel_', 'room_type_'], suffixes=('_act', '_tot'))\n",
    "    merged_df = merged_df.drop(['dow_', 'month_'], axis=1)\n",
    "\n",
    "    # Calculate demand probabilities and expected revenue\n",
    "    merged_df['probability'] = merged_df['total_rns'] / merged_df['total_rns_sum']\n",
    "    merged_df['expected_rns'] = merged_df['probability'] * merged_df['total_rns_median']\n",
    "    merged_df = merged_df.sort_values(by=['dow', 'month', 'adr'], ascending=[True, True, False])\n",
    "    merged_df['expected_demand'] = merged_df.groupby(['dow', 'month'])['expected_rns'].cumsum()\n",
    "    merged_df['expected_rev'] = merged_df['adr'] * merged_df['expected_demand']\n",
    "\n",
    "    # Initialize variables for iteration over months and days of the week\n",
    "    months = merged_df.month.unique()\n",
    "    dow = merged_df.dow.unique()\n",
    "\n",
    "    # Loop through hotels, room types, months, and days of the week to calculate optimal prices\n",
    "    for hotel in hotel_types:\n",
    "        for room_type in room_types:\n",
    "            for month in months:\n",
    "                for day in dow:\n",
    "                    data_subset = merged_df[(merged_df['dow'] == day) & (merged_df['hotel'] == hotel) & (merged_df['room_type'] == room_type) & (merged_df['month'] == month)].reset_index()\n",
    "                    if data_subset.empty:\n",
    "                        continue\n",
    "\n",
    "                    # Filter out outliers using Z-scores\n",
    "                    mean = data_subset['adr'].mean()\n",
    "                    std_dev = data_subset['adr'].std()\n",
    "                    data_subset['z_scores'] = np.abs((data_subset['adr'] - mean) / std_dev)\n",
    "                    data_subset = data_subset[data_subset['z_scores'] <= 2]\n",
    "\n",
    "                    # Prepare data for curve fitting to estimate demand curve parameters\n",
    "                    x_data = data_subset['adr'].values\n",
    "                    y_data = data_subset['expected_demand'].values\n",
    "\n",
    "                    try:\n",
    "                        initial_guess = [1, 0.01, 1, 1, data_subset['total_rns_median'].values[0]]\n",
    "                        bounds = ([0, 0, 0, 0, 0], [np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "                        maxfev = 10000\n",
    "                        params, _ = curve_fit(demand_curve, x_data, y_data, bounds=bounds, p0=initial_guess, maxfev=maxfev)\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"Error fitting demand curve for {hotel}, {room_type}, {month}, {day}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    a_fit, b_fit, c_fit, d_fit, max_demand = params\n",
    "\n",
    "                    # Calculate optimal price and expected revenue\n",
    "                    objective = lambda price: -revenue(price)\n",
    "                    optimize = minimize_scalar(objective, bounds=(45, 200), method='bounded')\n",
    "                    optimal_price = optimize.x\n",
    "                    max_revenue = -optimize.fun\n",
    "                    expected_rns = demand_curve(optimal_price, a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "                    optimal_rate_lim_inv = demand_to_price(data_subset['room_limit'].mean(), a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "\n",
    "                    # Add the results to the DataFrame\n",
    "                    new_row = pd.DataFrame({'hotel': hotel, 'room_type': room_type, 'room_limit': data_subset['room_limit'].mean(), 'month': month, 'dow': day, 'optimal_rate': optimal_price, 'expected_rev': max_revenue, 'expected_rn': expected_rns, 'optimal_rate_lim_inv': optimal_rate_lim_inv}, index=[0])\n",
    "                    results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "    # Round off final results\n",
    "    results['optimal_rate'] = results['optimal_rate'].round()\n",
    "    results['optimal_rate_lim_inv'] = results['optimal_rate_lim_inv'].round()\n",
    "    results['expected_rn'] = results['expected_rn'].round().astype(int)\n",
    "    results['expected_rev'] = results['expected_rev'].round()\n",
    "\n",
    "    # Generate date ranges and combinations of hotel types and room types for further modeling\n",
    "    combinations = list(itertools.product(hotel_types, room_types))\n",
    "    combinations_df = pd.DataFrame(combinations, columns=['hotel', 'room_type'])\n",
    "    month_dict = {month: index for index, month in enumerate(pd.date_range('2020-01-01', periods=12, freq='M').strftime('%B'), 1)}\n",
    "\n",
    "    new_data = pd.DataFrame()\n",
    "\n",
    "    # Create time series data for each combination of hotel and room type across multiple years\n",
    "    for year in range(2020, 2024):\n",
    "        for month in month_dict.values():\n",
    "            start_date = pd.to_datetime(f'{year}-{month}-01').replace(day=1)\n",
    "            end_date = pd.to_datetime(f'{year}-{month}-01').replace(day=1) + pd.offsets.MonthEnd(0)\n",
    "            date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "            df = pd.DataFrame(date_range, columns=['arrival_date'])\n",
    "            df['dow'] = df['arrival_date'].dt.day_name()\n",
    "            df['month'] = df['arrival_date'].dt.month_name()\n",
    "\n",
    "            # Merge the date data with hotel/room type combinations\n",
    "            result_df = df.assign(key=1).merge(combinations_df.assign(key=1), on='key').drop('key', axis=1)\n",
    "            new_data = pd.concat([new_data, result_df], ignore_index=True)\n",
    "\n",
    "    # Finalize the data by merging with the calculated results\n",
    "    final_data = pd.merge(new_data, results, how='left', on=['dow', 'hotel', 'room_type', 'month'])\n",
    "    final_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    final_data = final_data.dropna()\n",
    "    final_data['room_limit'] = final_data['room_limit'].astype(int)\n",
    "    final_data['arrival_date'] = pd.to_datetime(final_data['arrival_date']).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Write the final modeled data back to Snowflake\n",
    "    df_model = session.createDataFrame(final_data.values.tolist(), schema=final_data.columns.tolist())\n",
    "    df_model.write.mode(\"overwrite\").save_as_table(\"TTH_DB.TTH_REV_OPT_Schema.PROCESSED_PRICING_DATA\")\n",
    "\n",
    "    # Expand the booking data to one row per stay date\n",
    "    expanded_dfc1 = pd.DataFrame()\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        num_stay_dates = row['total_rns']\n",
    "        try:\n",
    "            # Create a row for each stay date\n",
    "            expanded_bookingc1 = pd.DataFrame({\n",
    "                'hotel': row['hotel'],\n",
    "                'room_type': row['reserved_room_type'], \n",
    "                'market_segment': row['market_segment'],\n",
    "                'deposit_type':row['deposit_type'],\n",
    "                'meal': row['meal'],\n",
    "                'distribution_channel': row['distribution_channel'],\n",
    "                'customer_type': row['customer_type'],\n",
    "                'arrival_date': pd.date_range(start=row['arrival_date_transformed'], periods=num_stay_dates),\n",
    "                'total_rns': 1,\n",
    "                'adr': row['adr'],\n",
    "                'room_limit': row['room_limit']\n",
    "            })\n",
    "            \n",
    "            # Append the stay date information to the new dataframe\n",
    "            expanded_dfc1 = pd.concat([expanded_dfc1, expanded_bookingc1], ignore_index=True)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing booking for {row['hotel']} on {row['arrival_date_transformed']} : {num_stay_dates} {e}\")\n",
    "    \n",
    "    # Final processing of booking frequency and writing back to Snowflake\n",
    "    expanded_dfc1 = expanded_dfc1.sort_values('arrival_date')\n",
    "    expanded_dfc1 = expanded_dfc1.reset_index(drop=True)\n",
    "    expanded_dfc1['adr']= np.round(expanded_dfc1['adr'], 2)\n",
    "    expanded_dfc1['dow'] = expanded_dfc1.arrival_date.dt.strftime('%A')\n",
    "    expanded_dfc1['month'] = expanded_dfc1.arrival_date.dt.strftime('%B')\n",
    "    \n",
    "    booking_frequency = expanded_dfc1.groupby(['hotel','room_type','market_segment','deposit_type','meal',\n",
    "                                            'distribution_channel','customer_type',\n",
    "                                               'arrival_date'], as_index=False, sort=True).agg({'adr': 'mean','room_limit': 'mean',\n",
    "                                                                    'total_rns': 'sum'})\n",
    "    booking_frequency = booking_frequency.sort_values(by=['arrival_date','hotel','room_type'], ascending=[True, True, True])\n",
    "    \n",
    "    # Write booking frequency data back to Snowflake\n",
    "    booking_frequency[\"arrival_date\"] = pd.to_datetime(booking_frequency[\"arrival_date\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_inter=session.createDataFrame(\n",
    "            booking_frequency.values.tolist(),\n",
    "            schema=booking_frequency.columns.tolist())\n",
    "    df_inter.write.mode(\"overwrite\").save_as_table(\"TTH_DB.TTH_REV_OPT_Schema.booking_frequency\")\n",
    "\n",
    "    return \"Data processed and dumped back to Snowflake\"\n",
    "$$\n",
    "''').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56803dc-7d47-431e-ae21-ba4661de07a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Session' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmy_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Session' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "my_session.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3055f29-be61-40a7-a6ee-13c00ba1a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hotel', 'is_canceled', 'lead_time', 'arrival_date_year', 'month',\n",
      "       'arrival_date_week_number', 'arrival_date_day_of_month',\n",
      "       'expected_arrival_date', 'reservation_status',\n",
      "       'reservation_status_date', 'total_stay_nights', 'tally_days',\n",
      "       'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children',\n",
      "       'babies', 'total_guests', 'avg_rooms_per_night', 'total_room_nights',\n",
      "       'meal', 'country', 'market_segment', 'distribution_channel',\n",
      "       'previous_cancellations', 'previous_bookings_not_canceled',\n",
      "       'reserved_room_type', 'assigned_room_type', 'deposit_type',\n",
      "       'days_in_waiting_list', 'customer_type', 'adr',\n",
      "       'arrival_date_transformed', 'reservation_status_date_transformed'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 187\u001b[0m\n\u001b[1;32m    182\u001b[0m     df_inter\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave_as_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTH_DB.TTH_REV_OPT_Schema.booking_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData processed and dumped back to Snowflake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 187\u001b[0m \u001b[43mcreate_pricing_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBOOKINGS_TRANSFORMED\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36mcreate_pricing_model\u001b[0;34m(session, table_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(expanded_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Aggregate daily room numbers and merge with ADR frequency data\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m daily_rns \u001b[38;5;241m=\u001b[39m \u001b[43mexpanded_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marrival_date_transformed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhotel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroom_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroom_limit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_rns\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     38\u001b[0m daily_rns \u001b[38;5;241m=\u001b[39m daily_rns\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrival_date_week_number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrival_date_day_of_month\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhotel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroom_type\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroom_limit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_rns\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m]})\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     39\u001b[0m daily_rns\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m daily_rns\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py:9156\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9159\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9162\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dow'"
     ]
    }
   ],
   "source": [
    "def create_pricing_model(session, table_name=None):\n",
    "\n",
    "    # Define the revenue function based on price and demand curve\n",
    "    def revenue(price):\n",
    "        return price * demand_curve(price, a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "\n",
    "    # Define the demand curve as an exponential function\n",
    "    def demand_curve(x, a, b, c, d, max_demand):\n",
    "        demand = a * np.exp(-b * x) + c\n",
    "        demand = np.where(x <= max_demand, np.minimum(demand, max_demand), demand)\n",
    "        return demand + d\n",
    "\n",
    "    # Calculate price based on the number of rooms\n",
    "    def demand_to_price(num_rooms, a, b, c, d, max_demand):\n",
    "        def root_func(x):\n",
    "            return num_rooms - (a * np.exp(-b * x) + c)\n",
    "\n",
    "        try:\n",
    "            price = brentq(root_func, 0, 200)\n",
    "        except ValueError:\n",
    "            price_range = (0, 200)\n",
    "            price = np.random.uniform(*price_range)\n",
    "\n",
    "        return price\n",
    "\n",
    "    # Retrieve data from Snowflake table and transform it to a Pandas DataFrame\n",
    "    expanded_df = session.table([table_name]).to_pandas()\n",
    "    expanded_df.columns = map(lambda x: str(x).lower(), expanded_df.columns)\n",
    "\n",
    "    # Initialize an empty DataFrame for storing results\n",
    "    results = pd.DataFrame(columns=['month', 'hotel', 'room_limit', 'room_type', 'dow', 'optimal_rate', 'expected_rn', 'expected_rev', 'optimal_rate_lim_inv'])\n",
    "    \n",
    "    hotel_types = ['Resort Hotel', 'City Hotel']\n",
    "    room_types = ['A', 'D', 'E']\n",
    "    print(expanded_df.columns)\n",
    "    # Aggregate daily room numbers and merge with ADR frequency data\n",
    "    daily_rns = expanded_df.groupby(['arrival_date_transformed', 'dow', 'month', 'hotel', 'room_type']).agg({'room_limit': 'mean', 'total_rns': 'sum'}).reset_index()\n",
    "    daily_rns = daily_rns.groupby(['arrival_date_week_number', 'arrival_date_day_of_month', 'hotel', 'room_type']).agg({'room_limit': 'mean', 'total_rns': ['sum', 'mean', 'median']}).reset_index()\n",
    "    daily_rns.columns = ['_'.join(col) for col in daily_rns.columns]\n",
    "    \n",
    "    adr_frequency = expanded_df.groupby(['dow', 'month', 'adr', 'hotel', 'room_type']).agg({'room_limit': 'mean', 'total_rns': 'sum'})\n",
    "    adr_frequency.reset_index(inplace=True)\n",
    "    \n",
    "    merged_df = pd.merge(adr_frequency, daily_rns, how='left', left_on=['dow', 'month', 'hotel', 'room_type'], right_on=['dow_', 'month_', 'hotel_', 'room_type_'], suffixes=('_act', '_tot'))\n",
    "    merged_df = merged_df.drop(['dow_', 'month_'], axis=1)\n",
    "\n",
    "    # Calculate demand probabilities and expected revenue\n",
    "    merged_df['probability'] = merged_df['total_rns'] / merged_df['total_rns_sum']\n",
    "    merged_df['expected_rns'] = merged_df['probability'] * merged_df['total_rns_median']\n",
    "    merged_df = merged_df.sort_values(by=['dow', 'month', 'adr'], ascending=[True, True, False])\n",
    "    merged_df['expected_demand'] = merged_df.groupby(['dow', 'month'])['expected_rns'].cumsum()\n",
    "    merged_df['expected_rev'] = merged_df['adr'] * merged_df['expected_demand']\n",
    "\n",
    "    # Initialize variables for iteration over months and days of the week\n",
    "    months = merged_df.month.unique()\n",
    "    dow = merged_df.dow.unique()\n",
    "\n",
    "    # Loop through hotels, room types, months, and days of the week to calculate optimal prices\n",
    "    for hotel in hotel_types:\n",
    "        for room_type in room_types:\n",
    "            for month in months:\n",
    "                for day in dow:\n",
    "                    data_subset = merged_df[(merged_df['dow'] == day) & (merged_df['hotel'] == hotel) & (merged_df['room_type'] == room_type) & (merged_df['month'] == month)].reset_index()\n",
    "                    if data_subset.empty:\n",
    "                        continue\n",
    "\n",
    "                    # Filter out outliers using Z-scores\n",
    "                    mean = data_subset['adr'].mean()\n",
    "                    std_dev = data_subset['adr'].std()\n",
    "                    data_subset['z_scores'] = np.abs((data_subset['adr'] - mean) / std_dev)\n",
    "                    data_subset = data_subset[data_subset['z_scores'] <= 2]\n",
    "\n",
    "                    # Prepare data for curve fitting to estimate demand curve parameters\n",
    "                    x_data = data_subset['adr'].values\n",
    "                    y_data = data_subset['expected_demand'].values\n",
    "\n",
    "                    try:\n",
    "                        initial_guess = [1, 0.01, 1, 1, data_subset['total_rns_median'].values[0]]\n",
    "                        bounds = ([0, 0, 0, 0, 0], [np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "                        maxfev = 10000\n",
    "                        params, _ = curve_fit(demand_curve, x_data, y_data, bounds=bounds, p0=initial_guess, maxfev=maxfev)\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"Error fitting demand curve for {hotel}, {room_type}, {month}, {day}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    a_fit, b_fit, c_fit, d_fit, max_demand = params\n",
    "\n",
    "                    # Calculate optimal price and expected revenue\n",
    "                    objective = lambda price: -revenue(price)\n",
    "                    optimize = minimize_scalar(objective, bounds=(45, 200), method='bounded')\n",
    "                    optimal_price = optimize.x\n",
    "                    max_revenue = -optimize.fun\n",
    "                    expected_rns = demand_curve(optimal_price, a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "                    optimal_rate_lim_inv = demand_to_price(data_subset['room_limit'].mean(), a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "\n",
    "                    # Add the results to the DataFrame\n",
    "                    new_row = pd.DataFrame({'hotel': hotel, 'room_type': room_type, 'room_limit': data_subset['room_limit'].mean(), 'month': month, 'dow': day, 'optimal_rate': optimal_price, 'expected_rev': max_revenue, 'expected_rn': expected_rns, 'optimal_rate_lim_inv': optimal_rate_lim_inv}, index=[0])\n",
    "                    results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "    # Round off final results\n",
    "    results['optimal_rate'] = results['optimal_rate'].round()\n",
    "    results['optimal_rate_lim_inv'] = results['optimal_rate_lim_inv'].round()\n",
    "    results['expected_rn'] = results['expected_rn'].round().astype(int)\n",
    "    results['expected_rev'] = results['expected_rev'].round()\n",
    "\n",
    "    # Generate date ranges and combinations of hotel types and room types for further modeling\n",
    "    combinations = list(itertools.product(hotel_types, room_types))\n",
    "    combinations_df = pd.DataFrame(combinations, columns=['hotel', 'room_type'])\n",
    "    month_dict = {month: index for index, month in enumerate(pd.date_range('2020-01-01', periods=12, freq='M').strftime('%B'), 1)}\n",
    "\n",
    "    new_data = pd.DataFrame()\n",
    "\n",
    "    # Create time series data for each combination of hotel and room type across multiple years\n",
    "    for year in range(2020, 2024):\n",
    "        for month in month_dict.values():\n",
    "            start_date = pd.to_datetime(f'{year}-{month}-01').replace(day=1)\n",
    "            end_date = pd.to_datetime(f'{year}-{month}-01').replace(day=1) + pd.offsets.MonthEnd(0)\n",
    "            date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "            df = pd.DataFrame(date_range, columns=['arrival_date'])\n",
    "            df['dow'] = df['arrival_date'].dt.day_name()\n",
    "            df['month'] = df['arrival_date'].dt.month_name()\n",
    "\n",
    "            # Merge the date data with hotel/room type combinations\n",
    "            result_df = df.assign(key=1).merge(combinations_df.assign(key=1), on='key').drop('key', axis=1)\n",
    "            new_data = pd.concat([new_data, result_df], ignore_index=True)\n",
    "\n",
    "    # Finalize the data by merging with the calculated results\n",
    "    final_data = pd.merge(new_data, results, how='left', on=['dow', 'hotel', 'room_type', 'month'])\n",
    "    final_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    final_data = final_data.dropna()\n",
    "    final_data['room_limit'] = final_data['room_limit'].astype(int)\n",
    "    final_data['arrival_date'] = pd.to_datetime(final_data['arrival_date']).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Write the final modeled data back to Snowflake\n",
    "    df_model = session.createDataFrame(final_data.values.tolist(), schema=final_data.columns.tolist())\n",
    "    df_model.write.mode(\"overwrite\").save_as_table(\"TTH_DB.TTH_REV_OPT_Schema.PROCESSED_PRICING_DATA\")\n",
    "\n",
    "    # Expand the booking data to one row per stay date\n",
    "    expanded_dfc1 = pd.DataFrame()\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        num_stay_dates = row['total_rns']\n",
    "        try:\n",
    "            # Create a row for each stay date\n",
    "            expanded_bookingc1 = pd.DataFrame({\n",
    "                'hotel': row['hotel'],\n",
    "                'room_type': row['reserved_room_type'], \n",
    "                'market_segment': row['market_segment'],\n",
    "                'deposit_type':row['deposit_type'],\n",
    "                'meal': row['meal'],\n",
    "                'distribution_channel': row['distribution_channel'],\n",
    "                'customer_type': row['customer_type'],\n",
    "                'arrival_date': pd.date_range(start=row['arrival_date_transformed'], periods=num_stay_dates),\n",
    "                'total_rns': 1,\n",
    "                'adr': row['adr'],\n",
    "                'room_limit': row['room_limit']\n",
    "            })\n",
    "            \n",
    "            # Append the stay date information to the new dataframe\n",
    "            expanded_dfc1 = pd.concat([expanded_dfc1, expanded_bookingc1], ignore_index=True)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing booking for {row['hotel']} on {row['arrival_date_transformed']} : {num_stay_dates} {e}\")\n",
    "    \n",
    "    # Final processing of booking frequency and writing back to Snowflake\n",
    "    expanded_dfc1 = expanded_dfc1.sort_values('arrival_date')\n",
    "    expanded_dfc1 = expanded_dfc1.reset_index(drop=True)\n",
    "    expanded_dfc1['adr']= np.round(expanded_dfc1['adr'], 2)\n",
    "    expanded_dfc1['dow'] = expanded_dfc1.arrival_date.dt.strftime('%A')\n",
    "    expanded_dfc1['month'] = expanded_dfc1.arrival_date.dt.strftime('%B')\n",
    "    \n",
    "    booking_frequency = expanded_dfc1.groupby(['hotel','room_type','market_segment','deposit_type','meal',\n",
    "                                            'distribution_channel','customer_type',\n",
    "                                               'arrival_date'], as_index=False, sort=True).agg({'adr': 'mean','room_limit': 'mean',\n",
    "                                                                    'total_rns': 'sum'})\n",
    "    booking_frequency = booking_frequency.sort_values(by=['arrival_date','hotel','room_type'], ascending=[True, True, True])\n",
    "    \n",
    "    # Write booking frequency data back to Snowflake\n",
    "    booking_frequency[\"arrival_date\"] = pd.to_datetime(booking_frequency[\"arrival_date\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_inter=session.createDataFrame(\n",
    "            booking_frequency.values.tolist(),\n",
    "            schema=booking_frequency.columns.tolist())\n",
    "    df_inter.write.mode(\"overwrite\").save_as_table(\"TTH_DB.TTH_REV_OPT_Schema.booking_frequency\")\n",
    "\n",
    "    return \"Data processed and dumped back to Snowflake\"\n",
    "\n",
    "\n",
    "create_pricing_model(my_session, \"BOOKINGS_TRANSFORMED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d162b-fe17-40f7-a7eb-9e5abc8f1fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
