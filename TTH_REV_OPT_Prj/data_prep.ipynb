{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8040318b",
   "metadata": {},
   "source": [
    "# install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d7206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/ab/e3c039b5ddba9335bd8f82d599eb310de1d2a2db0411b8d804d507405c74/pip-24.1.1-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "Successfully installed pip-24.1.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting snowflake-snowpark-python==1.9.0\n",
      "  Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fosforio\n",
      "  Downloading fosforio-1.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fosforml\n",
      "  Downloading fosforml-1.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting python-dateutil\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting holidays\n",
      "  Downloading holidays-0.52-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading snowflake_connector_python-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m246.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setuptools>=40.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting wheel (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.1.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyyaml (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cffi<2.0.0,>=1.9 (from snowflake-connector-python[pandas])\n",
      "  Downloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting cryptography<43.0.0,>=3.1.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading cryptography-42.0.8-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting pyOpenSSL<25.0.0,>=16.2.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading pyOpenSSL-24.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyjwt<3.0.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pytz (from snowflake-connector-python[pandas])\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting requests<3.0.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting packaging (from snowflake-connector-python[pandas])\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from snowflake-connector-python[pandas])\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from snowflake-connector-python[pandas])\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting platformdirs<5.0.0,>=2.6.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tomlkit (from snowflake-connector-python[pandas])\n",
      "  Downloading tomlkit-0.12.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting urllib3<2.0.0,>=1.21.1 (from snowflake-connector-python[pandas])\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m219.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow (from snowflake-connector-python[pandas])\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "INFO: pip is looking at multiple versions of fosforml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fosforml\n",
      "  Downloading fosforml-1.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting requests-toolbelt==0.9.1 (from fosforml)\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting shutils==0.1.0 (from fosforml)\n",
      "  Downloading shutils-0.1.0.tar.gz (2.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting mosaic-utils (from fosforml)\n",
      "  Downloading mosaic_utils-1.0.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<2.0.0,>=1.21.1 (from snowflake-connector-python[pandas])\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m177.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting configparser (from shutils==0.1.0->fosforml)\n",
      "  Downloading configparser-7.0.0-py3-none-any.whl.metadata (5.4 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql (from shutils==0.1.0->fosforml)\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m238.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting six>=1.5 (from python-dateutil)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pycparser (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas])\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib)\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fosforio-1.0.2-py3-none-any.whl (20 kB)\n",
      "Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m167.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fosforml-1.0.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m170.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m315.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m191.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m243.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m215.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m251.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m312.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m287.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m263.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading holidays-0.52-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m171.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m232.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m303.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.7/444.7 kB\u001b[0m \u001b[31m323.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m279.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m307.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-42.0.8-cp37-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading fonttools-4.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m248.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m309.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m211.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m233.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m261.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m290.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m245.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-70.2.0-py3-none-any.whl (930 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m260.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading snowflake_connector_python-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m294.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mosaic_utils-1.0.2-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m273.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\n",
      "Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m218.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Downloading configparser-7.0.0-py3-none-any.whl (16 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m248.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m157.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: shutils\n",
      "  Building wheel for shutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for shutils: filename=shutils-0.1.0-py3-none-any.whl size=3280 sha256=59c0ae2147a8bb21874ed35da8698d801c0a23206e159bfe5e93d81def8bc467\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-alrmxr3h/wheels/c7/06/1a/4df9f03d511efd19a10f2086f67c38cf14757b555f4f24bd6e\n",
      "Successfully built shutils\n",
      "Installing collected packages: sortedcontainers, pytz, asn1crypto, zipp, wheel, urllib3, tzdata, typing-extensions, tqdm, tomlkit, threadpoolctl, six, setuptools, pyyaml, pyparsing, pymysql, pyjwt, pycparser, platformdirs, pillow, packaging, nvidia-nccl-cu12, numpy, kiwisolver, joblib, idna, fonttools, filelock, cycler, configparser, cloudpickle, charset-normalizer, certifi, shutils, scipy, requests, python-dateutil, pyarrow, importlib-resources, contourpy, cffi, xgboost, scikit-learn, requests-toolbelt, pandas, matplotlib, holidays, cryptography, seaborn, pyOpenSSL, mosaic-utils, fosforio, snowflake-connector-python, fosforml, snowflake-snowpark-python\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.10.0 requires packaging<24, but you have packaging 24.1 which is incompatible.\n",
      "mlflow 2.10.0 requires pyarrow<16,>=4.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "mlflow 2.10.0 requires pytz<2024, but you have pytz 2024.1 which is incompatible.\n",
      "jupyterlab 3.2.4 requires jupyter-server~=1.4, but you have jupyter-server 2.0.0a1 which is incompatible.\n",
      "jupyterlab-server 2.25.4 requires jsonschema>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "mosaic-ai-client 1.0.0 requires matplotlib==3.1.1, but you have matplotlib 3.7.5 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires Flask==2.1.1; python_version >= \"3.7\", but you have flask 2.3.3 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires itsdangerous==2.0.1, but you have itsdangerous 2.2.0 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires Jinja2==3.0.3, but you have jinja2 3.1.4 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires matplotlib==3.6.0; python_version >= \"3.8\", but you have matplotlib 3.7.5 which is incompatible.\n",
      "openapi-schema-validator 0.6.2 requires jsonschema<5.0.0,>=4.19.1, but you have jsonschema 3.2.0 which is incompatible.\n",
      "openapi-spec-validator 0.7.1 requires jsonschema<5.0.0,>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires cryptography<39.0.0,>=3.1.0, but you have cryptography 42.0.8 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires packaging<24,>=20.9, but you have packaging 24.1 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires pandas<2,>=1.0.0, but you have pandas 2.0.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires xgboost<2,>=1.7.3, but you have xgboost 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asn1crypto-1.5.1 certifi-2024.6.2 cffi-1.16.0 charset-normalizer-3.3.2 cloudpickle-1.6.0 configparser-7.0.0 contourpy-1.1.1 cryptography-42.0.8 cycler-0.12.1 filelock-3.15.4 fonttools-4.53.0 fosforio-1.0.2 fosforml-1.0.0 holidays-0.52 idna-3.7 importlib-resources-6.4.0 joblib-1.4.2 kiwisolver-1.4.5 matplotlib-3.7.5 mosaic-utils-1.0.2 numpy-1.24.4 nvidia-nccl-cu12-2.22.3 packaging-24.1 pandas-2.0.0 pillow-10.4.0 platformdirs-4.2.2 pyOpenSSL-24.1.0 pyarrow-16.1.0 pycparser-2.22 pyjwt-2.8.0 pymysql-1.1.1 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytz-2024.1 pyyaml-6.0 requests-2.32.3 requests-toolbelt-0.9.1 scikit-learn-1.2.1 scipy-1.10.1 seaborn-0.13.2 setuptools-70.2.0 shutils-0.1.0 six-1.16.0 snowflake-connector-python-3.11.0 snowflake-snowpark-python-1.9.0 sortedcontainers-2.4.0 threadpoolctl-3.5.0 tomlkit-0.12.5 tqdm-4.66.4 typing-extensions-4.12.2 tzdata-2024.1 urllib3-1.26.15 wheel-0.43.0 xgboost-2.1.0 zipp-3.19.2\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.10.0 requires packaging<24, but you have packaging 24.1 which is incompatible.\n",
      "mlflow 2.10.0 requires pytz<2024, but you have pytz 2024.1 which is incompatible.\n",
      "fosforml 1.0.0 requires cloudpickle==1.6.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
      "fosforml 1.0.0 requires PyYAML==6.0, but you have pyyaml 6.0.1 which is incompatible.\n",
      "fosforml 1.0.0 requires urllib3==1.26.15, but you have urllib3 1.26.19 which is incompatible.\n",
      "jupyterlab 3.2.4 requires jupyter-server~=1.4, but you have jupyter-server 2.0.0a1 which is incompatible.\n",
      "jupyterlab-server 2.25.4 requires jsonschema>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "mosaic-ai-client 1.0.0 requires matplotlib==3.1.1, but you have matplotlib 3.7.5 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires Flask==2.1.1; python_version >= \"3.7\", but you have flask 2.3.3 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires itsdangerous==2.0.1, but you have itsdangerous 2.2.0 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires Jinja2==3.0.3, but you have jinja2 3.1.4 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires matplotlib==3.6.0; python_version >= \"3.8\", but you have matplotlib 3.7.5 which is incompatible.\n",
      "openapi-schema-validator 0.6.2 requires jsonschema<5.0.0,>=4.19.1, but you have jsonschema 3.2.0 which is incompatible.\n",
      "openapi-spec-validator 0.7.1 requires jsonschema<5.0.0,>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires cryptography<39.0.0,>=3.1.0, but you have cryptography 42.0.8 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires packaging<24,>=20.9, but you have packaging 24.1 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires pandas<2,>=1.0.0, but you have pandas 2.0.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires xgboost<2,>=1.7.3, but you have xgboost 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: urllib3 1.26.15\n",
      "Uninstalling urllib3-1.26.15:\n",
      "  Successfully uninstalled urllib3-1.26.15\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting urllib3==1.26.15\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fosforml 1.0.0 requires cloudpickle==1.6.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "fosforml 1.0.0 requires PyYAML==6.0, but you have pyyaml 6.0.1 which is incompatible.\n",
      "snowflake-snowpark-python 1.9.0 requires cloudpickle<=2.0.0,>=1.6.0; python_version < \"3.11\", but you have cloudpickle 2.2.1 which is incompatible.\n",
      "jupyterlab-server 2.25.4 requires jsonschema>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "mosaic-ai-client 1.0.0 requires matplotlib==3.1.1, but you have matplotlib 3.7.5 which is incompatible.\n",
      "openapi-spec-validator 0.7.1 requires jsonschema<5.0.0,>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires cryptography<39.0.0,>=3.1.0, but you have cryptography 42.0.8 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires pandas<2,>=1.0.0, but you have pandas 2.0.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires xgboost<2,>=1.7.3, but you have xgboost 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-1.26.15\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"snowflake-connector-python[pandas]\" \"snowflake-snowpark-python[pandas]\" snowflake-snowpark-python==1.9.0 fosforio fosforml numpy pandas matplotlib scikit-learn xgboost seaborn python-dateutil tqdm holidays\n",
    "!pip install --upgrade --q snowflake-snowpark-python==1.9.0\n",
    "!pip uninstall urllib3 -y\n",
    "!pip install urllib3==1.26.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef66af3",
   "metadata": {},
   "source": [
    "# Import helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983cfe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection manager service url initialised to http://fdc-project-manager:80/project-manager\n",
      "If you need to update its value then update the variable CONNECTION_MANAGER_BASE_URL in os env.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/301f4977-676a-4c39-9ca7-6a54313650a1/3.8/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-jf29s56z because the default path (/home/mosaic-ai/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from fosforio import snowflake\n",
    "from fosforml import *\n",
    "from fosforml.constants import MLModelFlavours\n",
    "from fosforio import get_dataframe\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from joblib import dump, load\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "from time import sleep\n",
    "import configparser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from dateutil.easter import easter\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c7d04",
   "metadata": {},
   "source": [
    "# connect to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53efc342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection details fetched: {'params': {'READER': {'connectionId': '73423620-7e1f-47a2-b9c5-b37408757323', 'connectionSources': {'sourceId': '210d73c1-4da7-487d-8a5c-05c8d2025269', 'connectionType': 'SNOWFLAKE', 'sourceName': 'RDBMS', 'enable': 1, 'displayName': 'SNOWFLAKE', 'displayOrder': 5.0}, 'createdBy': 'ms.akshaya@fosfor.com', 'createdOn': '2024-05-24T10:53:33.084+00:00', 'updatedOn': '2024-06-27T06:58:44.349+00:00', 'name': 'TTH_REV_OPT_CXN', 'autosyncable': False, 'autopublishable': False, 'frequency': None, 'assetConfigurations': [], 'publishableNodes': [], 'autoPublishableNodes': None, 'isPrivate': False, 'isExpired': False, 'autoSyncFrequency': None, 'application': None, 'sourceDataRefreshFrequency': None, 'defaultDb': 'TTH_DB', 'defaultSchema': 'TTH_REV_OPT_SCHEMA', 'recommendedPrivileges': {'DATA_MODULE': {'usage_on_schema': True, 'usage_on_database': True, 'create_table_on_schema': True, 'select_insert_update_truncate_delete_ref_on_tables': True}, 'INSIGHT_MODULE': {}, 'DECISION_MODULE': {'grant_on_warehouse_usage_to_role': True, 'grant_on_schema_to_role': True, 'create_stage_internal': True, 'create_udfs': True, 'grant_on_database_to_role': True, 'create_store_procedure': True}}, 'roleDetails': {'selectedStandardWarehouse': 'FOSFOR_SOLUTIONS_WH', 'selectedSnowparkOptimized': '', 'standardWarehouse': ['FOSFOR_SOLUTIONS_WH'], 'snowpark_optimized_warehouse': []}, 'dbPassword': 'Password@2023', 'port': None, 'ipAddress': None, 'dbUserName': 'Akshaya', 'sslEnable': False, 'token': None, 'client_secret': None, 'client_id': None, 'accountName': 'ug94937', 'region': 'us-east4', 'wareHouse': 'FOSFOR_SOLUTIONS_WH', 'cloudPlatform': 'gcp', 'role': 'AKSHAYA', 'privateKey': None, 'passPhrase': None, 'authenticationType': 'dbPassword', 'clientId': None, 'clientSecret': None, 'tokenUrl': None, 'scopeUrl': None, 'connectionUrl': 'https://ug94937.us-east4.gcp.snowflakecomputing.com'}}}\n",
      "Ex: No module named 'cryptography.__about__'\n",
      "Exception occurred in getting snowflake connection: Exception occurred in creating snowflake connection: None\n",
      "Reading dataframe from snowflake native connector\n",
      "project_id: 3a4a9607-ca97-4909-9161-c812e7fd08ce\n",
      "Connection details fetched: {'params': {'READER': {'schema': 'TTH_REV_OPT_SCHEMA', 'cloudPlatform': 'gcp', 'role': '\"AKSHAYA\"', 'clientId': None, 'tokenUrl': None, 'scopeUrl': None, 'type': 'RDBMS', 'wareHouse': 'FOSFOR_SOLUTIONS_WH', 'accountId': 'ug94937', 'privateKey': None, 'password': 'Password@2023', 'database': 'TTH_DB', 'tables': 'BOOKINGS_TRANSFORMED', 'sub_type': 'SNOWFLAKE', 'subType': 'SNOWFLAKE', 'clientSecret': None, 'authenticationType': 'dbPassword', 'rowCount': 10, 'region': 'us-east4', 'user': 'Akshaya', 'applicationName': 'FOSFOR', 'passPhrase': None}}, 'url': 'http://connectors-backend-service/connectors/v1/connectors/metadata', 'access': True, 'errorMsg': None, 'values': {}}\n",
      "Exception occurred in get_dataframe: No module named 'cryptography.__about__'\n"
     ]
    }
   ],
   "source": [
    "snowflake.get_connection(connection_name=\"TTH_REV_OPT_CXN\")\n",
    "data = get_dataframe(\"BOOKINGS_TRANSFORMED\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838d5fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "data.columns = [x.lower() for x in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de05c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa4084",
   "metadata": {},
   "source": [
    "# removing Canceletions and no-shows and keep City hotel data only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[(data['is_canceled'] == 0) & (data['reservation_status'] !='No-Show')] \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d75d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['hotel','market_segment','reserved_room_type']).agg({'adr':'mean','reservation_status_date_transformed':'count'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[(df.market_segment != 'Complementary') ]\n",
    "data = data[(data.reserved_room_type == 'A') |(data.reserved_room_type == 'D') | (data.reserved_room_type == 'E')]\n",
    "data.reserved_room_type.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe078cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = data['hotel'].unique()\n",
    "room_types = data['reserved_room_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc594eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_rns'] = data['stays_in_week_nights'] + data['stays_in_weekend_nights']\n",
    "data_to_transform = data[['hotel','reserved_room_type','arrival_date_transformed','total_rns','adr']]\n",
    "data_to_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snowflake.snowpark.session import Session\n",
    "user = os.getenv(\"user\")\n",
    "warehouse = os.getenv(\"warehouse\")\n",
    "schema= os.getenv(\"schema\")\n",
    "database = os.getenv(\"database\")\n",
    "role =  os.getenv(\"role\")\n",
    "account =  os.getenv(\"account\")\n",
    "password= os.getenv(\"password\")\n",
    "\n",
    "connection_params = dict(user=user, \n",
    "                         password=password, \n",
    "                         account=account, \n",
    "                         warehouse=warehouse, \n",
    "                         database=database,\n",
    "                         schema=schema, \n",
    "                         role=role)\n",
    "\n",
    "session = Session.builder.configs(connection_params).create()\n",
    "\n",
    "session.sql('use warehouse {};'.format(warehouse)).collect()\n",
    "\n",
    "session.sql('use database {};'.format(database)).collect()\n",
    "\n",
    "session.sql('use schema {}.{};'.format(database, schema)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ef4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b8ba8a9",
   "metadata": {},
   "source": [
    "# Create a new dataframe to store the data by stay date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.DataFrame()\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    num_stay_dates = row['total_rns']\n",
    "    try:\n",
    "        # Create a row for each stay date\n",
    "        expanded_booking = pd.DataFrame({\n",
    "            'hotel': row['hotel'],\n",
    "            'room_type': row['reserved_room_type'], \n",
    "            'arrival_date': pd.date_range(start=row['expected_arrival_date'], periods=num_stay_dates),\n",
    "            'total_rns': 1,\n",
    "            'adr': row['adr']\n",
    "        })\n",
    "        \n",
    "        # Append the stay date information to the new dataframe\n",
    "        expanded_df = pd.concat([expanded_df, expanded_booking], ignore_index=True)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing booking for {row['hotel']} on {row['expected_arrival_date']} : {num_stay_dates} {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4df76",
   "metadata": {},
   "source": [
    "# Sort the final dataframe by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168594ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expanded_df.sort_values('arrival_date')\n",
    "expanded_df = expanded_df.reset_index(drop=True)\n",
    "expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['adr']= np.round(expanded_df['adr'], 2)\n",
    "\n",
    "expanded_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb8a52",
   "metadata": {},
   "source": [
    "# Building seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "holiday_dates = holidays.CountryHoliday('PT', years=[2020,2021,2022,2023])\n",
    "holidays = {\n",
    "    expected_arrival_date: name\n",
    "    for expected_arrival_date, name in holiday_dates.items()\n",
    "    if name in ['Ano Novo', 'Páscoa', 'Dia de Natal']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a11810",
   "metadata": {},
   "source": [
    "# rename holiday columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cc18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expanded_df.rename({'Ano Novo':'new_year','Páscoa':'easter','Dia de Natal':'christmas'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holiday_dates(start_year, end_year):\n",
    "    holidays = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        holidays[datetime.date(year, 1, 1)] = 'new_year'\n",
    "        easter_date = easter(year)\n",
    "        holidays[easter_date] = 'easter'\n",
    "        holidays[datetime.date(year, 12, 25)] = 'christmas'\n",
    "    return holidays\n",
    "\n",
    "holidays = generate_holiday_dates(2020, 2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ec526",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ecf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pre and post ranges for each holiday\n",
    "pre_range_offset = {'new_year': relativedelta(days=-1),\n",
    "                    'easter': relativedelta(days=-2),\n",
    "                    'christmas': relativedelta(days=-3)}\n",
    "\n",
    "post_range_offset = {'new_year': relativedelta(days=1),\n",
    "                     'easter': relativedelta(days=2),\n",
    "                     'christmas': relativedelta(days=3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafa047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for each holiday\n",
    "for holiday in holidays.values():\n",
    "    expanded_df[holiday] = 0\n",
    " \n",
    " # Set the holiday columns to 1 for matching dates\n",
    "for arrival_date, name in holidays.items():\n",
    "    expanded_df.loc[expanded_df['arrival_date'] == arrival_date, name] = 1\n",
    "\n",
    "    # Set the holiday columns to 1 for pre and post dates\n",
    "    pre_offset = pre_range_offset.get(name)\n",
    "    if pre_offset:\n",
    "        pre_date = pd.to_datetime(arrival_date) + pre_offset\n",
    "        expanded_df.loc[expanded_df['arrival_date'] == pre_date.strftime('%Y-%m-%d'), name] = 1\n",
    "\n",
    "    post_offset = post_range_offset.get(name)\n",
    "    if post_offset:\n",
    "        post_date = pd.to_datetime(arrival_date) + post_offset\n",
    "        expanded_df.loc[expanded_df['arrival_date'] == post_date.strftime('%Y-%m-%d'), name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d034b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##check\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ed484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dow, month to data\n",
    "expanded_df['dow'] = expanded_df.arrival_date.dt.strftime('%A')\n",
    "expanded_df['month'] = expanded_df.arrival_date.dt.strftime('%B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116caac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##check\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb3fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df[expanded_df['easter'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "\n",
    "def demand_to_price(num_rooms, a, b, c, d, max_demand, optimal_price):\n",
    "    def root_func(x):\n",
    "        return num_rooms - (a * np.exp(-b * x) + c)\n",
    "\n",
    "    try:\n",
    "        price = brentq(root_func, 0, 200)  # Adjust the interval bounds as needed\n",
    "    except ValueError:\n",
    "        # Fallback to default price if no root is found\n",
    "        price_range=(0, optimal_price)\n",
    "        price = np.random.uniform(*price_range)\n",
    "\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['adr'] = np.round(expanded_df['adr'], 2)\n",
    "expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1dfe5",
   "metadata": {},
   "source": [
    "# Non holidays dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8373c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_holidays = expanded_df[expanded_df[['new_year', 'easter', 'christmas']].sum(axis=1) == 0]\n",
    "\n",
    "non_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_holidays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_rns= non_holidays.groupby(['arrival_date','dow','month', 'hotel', 'room_type']).agg({'total_rns':'sum'}).reset_index() # ge total stays per day\n",
    "\n",
    "daily_rns = daily_rns.groupby(['dow','month', 'hotel', 'room_type']).agg({'total_rns':['sum','mean','median']}).reset_index() # get Rns metrics by Dow & Month\n",
    "\n",
    "daily_rns.columns = ['_'.join(col) for col in daily_rns.columns] #remove multi level column\n",
    "daily_rns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_frequency = non_holidays.groupby(['dow','month','adr', 'hotel', 'room_type']).agg({'total_rns':'sum'})\n",
    "adr_frequency.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(adr_frequency, daily_rns,how='left',left_on=['dow','month'], right_on=['dow_','month_'],suffixes=('_act', '_tot'))\n",
    "\n",
    "merged_df = merged_df.drop(['dow_','month_'],axis=1)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1355c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['probability'] = merged_df['total_rns']/merged_df['total_rns_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdaa3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['expected_rns'] = merged_df['probability'] * merged_df['total_rns_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba54f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(by=['dow', 'month', 'adr'], ascending=[True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78068d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['expected_demand']=merged_df.groupby(['dow', 'month'])['expected_rns'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['expected_rev'] = merged_df['adr']* merged_df['expected_demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['expected_rev'] = merged_df['adr']* merged_df['expected_demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.dow == 'Friday') & (merged_df.month =='April')].plot(x='adr', y='expected_demand', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c57bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.dow == 'Friday') & (merged_df.month =='April')].plot(x='adr', y='expected_rev', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define the demand curve function\n",
    "def demand_curve(x, a, b, c, d, max_demand):\n",
    "    demand = a * np.exp(-b * x) + c\n",
    "    demand = np.where(x <= max_demand, np.minimum(demand, max_demand), demand)\n",
    "    return demand + d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856be003",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = merged_df['adr'].values\n",
    "y_data = merged_df['expected_demand'].values\n",
    "\n",
    "\n",
    "initial_guess = [1, 0.01, 1, 1, 100]\n",
    "bounds = ([0, 0, 0, 0, 0], [np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "params, _ = curve_fit(demand_curve, x_data, y_data, bounds=bounds, p0=initial_guess)\n",
    "\n",
    "a_fit, b_fit, c_fit ,d_fit,max_demand= params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb449ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_demand = demand_curve(x_data, a_fit, b_fit,c_fit,d_fit,max_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02eb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_data, y_data, label='Actual Demand')\n",
    "plt.plot(x_data, predicted_demand, label='Fitted Curve')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Demand')\n",
    "plt.legend()\n",
    "plt.title('Demand Curve Fit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2582a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revenue(price):\n",
    "    return price * demand_curve(price, a_fit, b_fit,c_fit,d_fit,max_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = lambda price: -revenue(price)\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "result = minimize_scalar(objective, bounds=(60, 180), method='bounded')\n",
    "optimal_price = result.x\n",
    "max_revenue = -result.fun\n",
    "room_sold = demand_curve(optimal_price, a_fit, b_fit,c_fit,d_fit,max_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd335c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The optimal price to maximize revenue: ${optimal_price}\")\n",
    "print(f\"The maximum revenue achievable: ${max_revenue}\")\n",
    "print(f\"The expected number of rooms to sell: {room_sold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_to_price(50,a_fit,b_fit,c_fit,d_fit,max_demand, optimal_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['month', 'hotel', 'room_type', 'dow', 'optimal_rate', 'expected_rn','expected_rev','optimal_rate_lim_inv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = merged_df.month.unique()\n",
    "dow = merged_df.dow.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a loop to observe if our demand curve fits properly to each demand month and dow\n",
    "\n",
    "\n",
    "for month in months:\n",
    "    for day in dow:\n",
    "        \n",
    "        print(month,day)\n",
    "        #get data\n",
    "        data = merged_df[(merged_df.dow == day) & (merged_df.month ==month)].reset_index()\n",
    "        \n",
    "        #remove outlier\n",
    "        mean = data.adr.mean()\n",
    "        std_dev = data.adr.std()\n",
    "       \n",
    "\n",
    "        # calculate z-scores\n",
    "        data['z_scores'] = np.abs((data.adr - mean) / std_dev)\n",
    "        \n",
    "        #filter out outliers\n",
    "        data = data[data.z_scores <=2]\n",
    "        \n",
    "        ## Fit Demand curve\n",
    "        x_data = data['adr'].values\n",
    "        y_data = data['expected_demand'].values\n",
    "        \n",
    "        # Try except expression to ensure we get no errors when fitting the demand curve due to our initial guess\n",
    "        try:\n",
    "            initial_guess = [1, 0.01, 1, 1,data['total_rns_median'].values[0] ]\n",
    "            bounds = ([0, 0, 0, 0, 0], [np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "        # Fit the demand curve to the data\n",
    "            params, _ = curve_fit(demand_curve, x_data, y_data, bounds=bounds, p0=initial_guess)\n",
    "        except:\n",
    "            if month =='January':\n",
    "                \n",
    "                initial_guess = [1, 0.01, 1, 1,40 ]\n",
    "            else:\n",
    "                initial_guess = [1, 0.01, 1, 1,50 ]\n",
    "            bounds = ([0, 0, 0, 0, 0], [np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "        # Fit the demand curve to the data\n",
    "            params, _ = curve_fit(demand_curve, x_data, y_data, bounds=bounds, p0=initial_guess)\n",
    "        \n",
    "        # Extract the fitted parameters\n",
    "        a_fit, b_fit, c_fit ,d_fit,max_demand= params\n",
    "        \n",
    "        #visually explore if the demand curve fits the data\n",
    "        predicted_demand = demand_curve(x_data, a_fit, b_fit,c_fit,d_fit,max_demand)\n",
    "        \n",
    "        plt.scatter(x_data, y_data, label='Actual Demand')\n",
    "        plt.plot(x_data, predicted_demand, label='Fitted Curve')\n",
    "        plt.xlabel('Price')\n",
    "        plt.ylabel('Demand')\n",
    "        plt.legend()\n",
    "        plt.title('Demand Curve Fit')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c68db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hotel in hotels:\n",
    "    for room_type in room_types:\n",
    "        for month in months:\n",
    "            for day in dow:\n",
    "                # Get data for the specific combination\n",
    "                data_subset = merged_df[(merged_df['dow'] == day) & \n",
    "                                        (merged_df['hotel'] == hotel) & \n",
    "                                        (merged_df['room_type'] == room_type) & \n",
    "                                        (merged_df['month'] == month)].reset_index()\n",
    "                \n",
    "                if data_subset.empty:\n",
    "                    continue\n",
    "\n",
    "                # Remove outliers\n",
    "                mean = data_subset['adr'].mean()\n",
    "                std_dev = data_subset['adr'].std()\n",
    "                data_subset['z_scores'] = np.abs((data_subset['adr'] - mean) / std_dev)\n",
    "                data_subset = data_subset[data_subset['z_scores'] <= 2]\n",
    "\n",
    "                # Fit demand curve\n",
    "                x_data = data_subset['adr'].values\n",
    "                y_data = data_subset['expected_demand'].values\n",
    "\n",
    "                try:\n",
    "                    initial_guess = [1, 0.01, 1, 1, data_subset['total_rns_median'].values[0]]\n",
    "                    bounds = ([0, 0, 0, 0, 0], [np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "                    maxfev = 10000  # Increase the number of maximum function evaluations\n",
    "                    params, _ = curve_fit(demand_curve, x_data, y_data, bounds=bounds, p0=initial_guess, maxfev=maxfev)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error fitting demand curve for {hotel}, {room_type}, {month}, {day}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                a_fit, b_fit, c_fit, d_fit, max_demand = params\n",
    "\n",
    "                # Optimize revenue\n",
    "                def revenue(price):\n",
    "                    return price * demand_curve(price, a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "\n",
    "                objective = lambda price: -revenue(price)\n",
    "                optimize = minimize_scalar(objective, bounds=(45, 200), method='bounded')\n",
    "                optimal_price = optimize.x\n",
    "                max_revenue = -optimize.fun\n",
    "                expected_rns = demand_curve(optimal_price, a_fit, b_fit, c_fit, d_fit, max_demand)\n",
    "\n",
    "                optimal_rate_lim_inv = demand_to_price(50, a_fit, b_fit, c_fit, d_fit, max_demand, optimal_price)\n",
    "\n",
    "                new_row = pd.DataFrame({'hotel': hotel,\n",
    "                                        'room_type': room_type,\n",
    "                                        'month': month,\n",
    "                                        'dow': day,\n",
    "                                        'optimal_rate': optimal_price,\n",
    "                                        'expected_rev': max_revenue,\n",
    "                                        'expected_rn': expected_rns,\n",
    "                                        'optimal_rate_lim_inv': optimal_rate_lim_inv}, index=[0])\n",
    "                results = pd.concat([results, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fac15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa35f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Sort the dataframe by the custom order\n",
    "results['month'] = pd.Categorical(results['month'], categories=month_order, ordered=True)\n",
    "results['dow'] = pd.Categorical(results['dow'], categories=dow_order, ordered=True)\n",
    "results = results.sort_values(['month', 'dow'])\n",
    "\n",
    "grouped = results.groupby(['month', 'dow'])['optimal_rate'].mean().unstack()\n",
    "\n",
    "# Create the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "grouped.plot(ax=ax, kind='bar')\n",
    "ax.set_xlabel('Day of the Week')\n",
    "ax.set_ylabel('Optimal Rate')\n",
    "ax.set_title('Optimal Rate by Month and Day of the Week')\n",
    "\n",
    "# Customize the appearance (optional)\n",
    "plt.legend(title='Month', bbox_to_anchor=(1, 1))\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc29da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up the results\n",
    "results['optimal_rate'] = results['optimal_rate'].round()\n",
    "results['optimal_rate_lim_inv'] = results['optimal_rate_lim_inv'].round()\n",
    "\n",
    "results['expected_rn'] = results['expected_rn'].round().astype(int)\n",
    "results['expected_rev'] = results['expected_rev'].round()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67157075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d68756d4",
   "metadata": {},
   "source": [
    "# Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays =  expanded_df[expanded_df[['new_year', 'easter', 'christmas']].sum(axis=1) != 0]\n",
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted = pd.melt(holidays, id_vars=['arrival_date', 'total_rns', 'adr', 'dow', 'month', \"hotel\", \"room_type\"],\n",
    "                    value_vars=['new_year', 'easter', 'christmas'],\n",
    "                    var_name='holiday', value_name='holiday_indicator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted[unpivoted.holiday =='christmas']\n",
    "unpivoted = unpivoted[unpivoted['holiday_indicator'] == 1]\n",
    "unpivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_adr = unpivoted.groupby(['holiday','adr','holiday_indicator', \"hotel\", \"room_type\"]).agg({'total_rns':'sum'}).reset_index()\n",
    "holiday_rns = unpivoted.groupby(['arrival_date','holiday', \"hotel\", \"room_type\"]).agg({'total_rns':'sum'}).reset_index()\n",
    "holiday_rns = holiday_rns.groupby(['holiday', \"hotel\", \"room_type\"]).agg({'total_rns':['sum','mean','median']}).reset_index()\n",
    "holiday_rns.columns = ['_'.join(col) for col in holiday_rns.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a38166",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_rns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_holidays = pd.merge(holiday_adr, holiday_rns,how='left',left_on=['holiday'], right_on=['holiday_'],suffixes=('_act', '_tot'))\n",
    "\n",
    "merged_holidays.drop('holiday_',axis=1,inplace=True)\n",
    "\n",
    "merged_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa72a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_holidays['probability'] = merged_holidays['total_rns']/merged_holidays['total_rns_sum']\n",
    "merged_holidays['expected_rns'] = merged_holidays['probability'] * merged_holidays['total_rns_median']\n",
    "merged_holidays = merged_holidays.sort_values(by=['holiday', 'adr'], ascending=[True, False])\n",
    "merged_holidays['expected_demand']=merged_holidays.groupby(['holiday'])['expected_rns'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_holidays = merged_holidays.holiday.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f66b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demand_to_price(num_rooms, a, b, c, d, max_demand, optimal_price):\n",
    "    def root_func(x):\n",
    "        return num_rooms - (a * np.exp(-b * x) + c)\n",
    "\n",
    "    try:\n",
    "        price = brentq(root_func, 0, 200)  # Adjust the interval bounds as needed\n",
    "    except ValueError:\n",
    "        # Fallback to default price if no root is found\n",
    "        price_range=(0, optimal_price)\n",
    "        price = np.random.uniform(*price_range)\n",
    "\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0febbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_results = pd.DataFrame(columns=['holiday', 'optimal_rate', 'expected_rn','expected_rev','optimal_rate_lim_inv'])\n",
    "\n",
    "for hotel in hotels:\n",
    "    for room_type in room_types:\n",
    "\n",
    "        for day in unique_holidays:\n",
    "                data = merged_holidays[(merged_holidays.holiday == day) & (merged_holidays.hotel == hotel) & (merged_holidays.room_type == room_type) ].reset_index()\n",
    "\n",
    "                #remove outlier\n",
    "                mean = data.adr.mean()\n",
    "                std_dev = data.adr.std()\n",
    "\n",
    "\n",
    "                # calculate z-scores\n",
    "                data['z_scores'] = np.abs((data.adr - mean) / std_dev)\n",
    "\n",
    "                #filter out outliers\n",
    "                data = data[data.z_scores <=2]\n",
    "\n",
    "                ## Fit Demand curve\n",
    "                x_data = data['adr'].values\n",
    "                y_data = data['expected_demand'].values\n",
    "\n",
    "                initial_guess = [1, 0.01, 1, 1,data['total_rns_median'].values[0] ]\n",
    "                bounds = ([0, 0, 0, 0, 0], [np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "\n",
    "                try:\n",
    "                    params, _ = curve_fit(demand_curve, x_data, y_data, bounds=bounds, p0=initial_guess)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting demand curve for {hotel}, {room_type}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Extract the fitted parameters\n",
    "                a_fit, b_fit, c_fit ,d_fit,max_demand= params\n",
    "\n",
    "                a_fit, b_fit, c_fit ,d_fit,max_demand = np.round(a_fit, 3), np.round(b_fit, 3), np.round(c_fit, 3) ,np.round(d_fit, 3),np.round(max_demand, 3)\n",
    "                #optimze revenue\n",
    "                objective = lambda price: -revenue(price)\n",
    "\n",
    "\n",
    "\n",
    "                optimize = minimize_scalar(objective, bounds=(45, 400), method='bounded')\n",
    "                optimal_price = optimize.x\n",
    "                max_revenue = -optimize.fun\n",
    "\n",
    "                expected_rns = demand_curve(optimal_price,a_fit, b_fit, c_fit ,d_fit,max_demand)\n",
    "                print(50,a_fit,b_fit,c_fit,d_fit,max_demand)\n",
    "                optimal_rate_lim_inv = demand_to_price(50,a_fit,b_fit,c_fit,d_fit,max_demand, optimal_price)\n",
    "\n",
    "                new_row = pd.DataFrame({'holiday':day,\n",
    "                                        'hotel': hotel,\n",
    "                               'room_type': room_type,\n",
    "                               'optimal_rate': optimal_price,\n",
    "                               'expected_rev':max_revenue,\n",
    "                               'expected_rn':expected_rns,\n",
    "                               'optimal_rate_lim_inv':optimal_rate_lim_inv},index=[0])\n",
    "                holiday_results = pd.concat([holiday_results, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e864db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_results\n",
    "\n",
    "\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "\n",
    "holiday_dates = []\n",
    "for year in years:\n",
    "    for index, row in holiday_results.iterrows():\n",
    "        if row['holiday'] == 'christmas':\n",
    "            date = datetime.date(year, 12, 25)\n",
    "        elif row['holiday'] == 'easter':\n",
    "            date = easter(year)\n",
    "        elif row['holiday'] == 'new_year':\n",
    "            date = datetime.date(year, 1, 1)\n",
    "\n",
    "        holiday_dates.append({\n",
    "            'hotel': row['hotel'],\n",
    "            'room_type': row['room_type'],\n",
    "            'month': date.strftime('%B'),\n",
    "            'dow': date.strftime(\"%A\"),\n",
    "            'holiday': row['holiday'],\n",
    "            'optimal_rate': row['optimal_rate'],\n",
    "            'expected_rn': row['expected_rn'],\n",
    "            'expected_rev': row['expected_rev'],\n",
    "            'optimal_rate_lim_inv': row['optimal_rate_lim_inv'],\n",
    "            'arrival_date': pd.to_datetime(date)\n",
    "        })\n",
    "\n",
    "holiday_results_yearly = pd.DataFrame(holiday_dates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([results, holiday_results_yearly], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.drop('arrival_date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddefe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['holiday'].fillna('non_holiday', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5697c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([final_data, pd.get_dummies(final_data.holiday)], axis=1)\n",
    "final_data.drop('holiday', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef455da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_data.to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee610e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2020\n",
    "end_year = 2023\n",
    "\n",
    "month_dict = {month: index for index, month in enumerate(pd.date_range('2020-01-01', periods=12, freq='M').strftime('%B'), 1)}\n",
    "\n",
    "def generate_dates(row):\n",
    "    month_num = month_dict[row['month']]\n",
    "    dates = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        month_dates = pd.date_range(start=f'{year}-{month_num:02d}-01', end=f'{year}-{month_num:02d}-01', freq='D') + MonthEnd(0)\n",
    "        dow_dates = month_dates[month_dates.day_name() == row['dow']]\n",
    "        dates.extend(dow_dates)\n",
    "    return dates\n",
    "\n",
    "expanded_data = final_data.apply(lambda row: pd.DataFrame({\n",
    "    'month': row['month'],\n",
    "    'hotel': row['hotel'],\n",
    "    'room_type': row['room_type'],\n",
    "    'dow': row['dow'],\n",
    "    'optimal_rate': row['optimal_rate'],\n",
    "    'expected_rn': row['expected_rn'],\n",
    "    'expected_rev': row['expected_rev'],\n",
    "    'optimal_rate_lim_inv': row['optimal_rate_lim_inv'],\n",
    "    'christmas': row['christmas'],\n",
    "    'easter': row['easter'],\n",
    "    'new_year': row['new_year'],\n",
    "    'non_holiday': row['non_holiday'],\n",
    "    'arrival_date': generate_dates(row)\n",
    "}), axis=1).explode('arrival_date').reset_index(drop=True)\n",
    "\n",
    "expanded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dab23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a0b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efefb332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da7fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d67b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aefe7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cf678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
