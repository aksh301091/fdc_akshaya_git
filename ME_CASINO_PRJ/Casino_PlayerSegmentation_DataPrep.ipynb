{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ab1c46-64e0-48ab-abd1-85954d62531d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Casino Customer Segmentation Model Using Python and Snowflake\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates the process of building a customer segmentation model for a casino using Python and Snowflake. The segmentation allows the casino to categorize customers based on their behavioral patterns and tailor marketing strategies accordingly.\n",
    "\n",
    "### Objectives:\n",
    "- Retrieve customer and transaction data from Snowflake.\n",
    "- Perform data preprocessing and feature engineering.\n",
    "- Explore data through visualization.\n",
    "- Use machine learning to cluster customers into distinct groups.\n",
    "- Analyze and interpret each customer segment for actionable insights.\n",
    "\n",
    "## 1. Library Setup and Installation\n",
    "To start, we need to install and import the necessary Python libraries, including Snowflake connectors for data access and Pandas, Scikit-learn, and Matplotlib for data manipulation, machine learning, and visualization.\n",
    "\n",
    "### Purpose:\n",
    "Ensure that all required libraries are installed and configured for data ingestion, processing, and modeling.\n",
    "\n",
    "## 2. Data Import and Preprocessing\n",
    "This section involves loading customer and transaction data from the Snowflake database into Pandas DataFrames. After importing, the data undergoes initial cleaning and transformation.\n",
    "\n",
    "### Key Steps:\n",
    "- Import customer and transaction data.\n",
    "- Convert data into Pandas and Snowflake DataFrames.\n",
    "- Preprocess the data by handling missing values and transforming categorical variables.\n",
    "- Scale numerical data to ensure consistency across features.\n",
    "\n",
    "### Outcome:\n",
    "A clean and preprocessed dataset ready for further analysis.\n",
    "\n",
    "## 3. Feature Engineering\n",
    "Feature engineering is used to create additional attributes that provide more insights into customer behavior. These new features help improve the clustering and segmentation process.\n",
    "\n",
    "### Key Features:\n",
    "- **Visit Frequency**: How often a customer visits the casino, calculated from the total visits and time span between their first and last visit.\n",
    "- **Preferred Game**: The game where each customer spends the most time, providing insight into individual preferences.\n",
    "- **Revenue Metrics**: Total chips won or lost, giving a financial perspective on customer activity.\n",
    "\n",
    "### Purpose:\n",
    "Generate new features to better capture customer behavior, enhancing the clustering algorithm's effectiveness.\n",
    "\n",
    "## 4. Exploratory Data Analysis (EDA) and Visualization\n",
    "In this step, we use data visualization techniques to explore the relationships between variables and identify patterns in customer behavior. This helps in understanding the data before applying clustering.\n",
    "\n",
    "### Visualizations:\n",
    "- **Correlation Matrix**: Shows relationships between numerical features.\n",
    "- **Distribution Plots**: Visualize the spread of important variables like customer age, revenue, and visit frequency.\n",
    "- **Histograms**: Display the distribution of key variables and identify outliers.\n",
    "\n",
    "### Purpose:\n",
    "EDA helps uncover hidden trends and relationships in the data, guiding further analysis and machine learning tasks.\n",
    "\n",
    "## 5. Dimensionality Reduction and Clustering\n",
    "To reduce the complexity of the data, we apply Principal Component Analysis (PCA) and then use clustering algorithms such as K-Means and Agglomerative Clustering to group customers based on their behavior.\n",
    "\n",
    "### Steps:\n",
    "- **PCA**: Reduces the number of dimensions, simplifying the dataset while retaining important information.\n",
    "- **K-Means Clustering**: Groups customers into clusters based on their behavioral patterns.\n",
    "- **Agglomerative Clustering**: Another clustering technique used to hierarchically group customers.\n",
    "\n",
    "### Purpose:\n",
    "Dimensionality reduction makes the dataset easier to visualize, and clustering reveals natural groupings of customers.\n",
    "\n",
    "\n",
    "## 6. Cluster Profiling\n",
    "Once clusters are created, we profile each one to understand the characteristics of customers within each group. Profiling helps in labeling the clusters and identifying their unique behaviors.\n",
    "\n",
    "### Key Metrics:\n",
    "- **Total Revenue**: How much each cluster contributes to the casino's revenue.\n",
    "- **Visit Frequency**: The frequency with which customers in each cluster visit the casino.\n",
    "- **Preferred Games**: Which games are favored by different clusters.\n",
    "\n",
    "### Visualizations:\n",
    "- **3D Plot**: Visualizes clusters in a 3D space.\n",
    "- **Box Plots**: Compare how key metrics like age, revenue, and number of visits vary across clusters.\n",
    "\n",
    "### Outcome:\n",
    "Each cluster is assigned meaningful labels based on customer behavior, making it easier to target them with specific marketing strategies.\n",
    "\n",
    "## 7. Customer Segmentation\n",
    "Now that we have clustered the customers, we assign descriptive names to the segments to better understand and interpret the groups.\n",
    "\n",
    "### Example Segments:\n",
    "- **High Roller Professionals**: Customers who frequently visit and spend large amounts.\n",
    "- **Conservative Low Spenders**: Customers who visit infrequently and spend minimal amounts.\n",
    "- **Cross-Spending Players**: Customers who engage in various activities such as gaming, dining, and hotel stays.\n",
    "\n",
    "### Purpose:\n",
    "Assign meaningful labels to customer segments to help the casino personalize marketing and engagement strategies.\n",
    "\n",
    "## 8. Feature Interaction and Exploration\n",
    "This section explores how key features interact within the different clusters, providing deeper insights into customer behavior.\n",
    "\n",
    "### Example Interactions:\n",
    "- **Revenue vs. Visit Frequency**: A scatter plot showing the relationship between visit frequency and total revenue.\n",
    "- **Age vs. Cluster**: How age influences cluster membership.\n",
    "\n",
    "### Visualizations:\n",
    "- **Scatter Plots**: Show how key variables differ across clusters.\n",
    "- **Box Plots**: Compare the distribution of revenue and other features across clusters.\n",
    "\n",
    "### Purpose:\n",
    "Feature interactions provide insights into customer preferences and behaviors, enabling the casino to develop more tailored marketing strategies.\n",
    "\n",
    "## Summary\n",
    "In this notebook, we have successfully segmented casino customers into distinct groups based on their behavioral patterns. By analyzing these clusters, the casino can optimize its marketing efforts and offer personalized services to improve customer retention.\n",
    "\n",
    "### Key Takeaways:\n",
    "- We identified meaningful customer segments such as high rollers and low spenders.\n",
    "- Data visualization provided critical insights into customer behavior.\n",
    "- Clustering allowed us to uncover groups of customers with similar behavioral patterns.\n",
    "- These results can be used to enhance marketing campaigns and improve customer satisfaction.\n",
    "\n",
    "This approach provides a strong foundation for developing personalized marketing strategies that increase engagement and drive revenue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f5e35-d98f-42ac-a393-6e189bf16231",
   "metadata": {},
   "source": [
    "# Setup and Installation of Required Libraries\n",
    "\n",
    "### In this section, we install and configure all the necessary libraries for connecting to Snowflake, data manipulation, visualization, machine learning, and other utilities. We ensure all packages are up-to-date and compatible with the project requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92147d7e-59aa-40c5-81cb-368b714b09cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/55/90db48d85f7689ec6f81c0db0622d704306c5284850383c090e6c7195a5c/pip-24.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 10.3MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-24.2\n",
      "Collecting snowflake-snowpark-python==1.9.0\n",
      "  Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (2.2.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (4.66.4)\n",
      "Collecting holidays\n",
      "  Downloading holidays-0.57-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting faker\n",
      "  Downloading Faker-30.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading snowflake_connector_python-3.12.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (65 kB)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python==1.9.0) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python==1.9.0) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python==1.9.0) (4.12.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python==1.9.0) (6.0.1)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (42.0.5)\n",
      "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (24.0.0)\n",
      "Collecting pyjwt<3.0.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (2.32.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (2024.7.4)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]) (4.2.2)\n",
      "Collecting tomlkit (from snowflake-connector-python[pandas])\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting urllib3<2.0.0,>=1.21.1 (from snowflake-connector-python[pandas])\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Collecting pyarrow (from snowflake-connector-python[pandas])\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.19.2)\n",
      "Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl (327 kB)\n",
      "Downloading matplotlib-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m187.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m178.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading holidays-0.57-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Faker-30.1.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fonttools-4.54.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m156.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m139.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m210.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading snowflake_connector_python-3.12.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m144.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: sortedcontainers, asn1crypto, urllib3, tomlkit, threadpoolctl, scipy, pyparsing, pyjwt, pyarrow, pillow, nvidia-nccl-cu12, kiwisolver, joblib, importlib-resources, fonttools, filelock, cycler, contourpy, cloudpickle, xgboost, scikit-learn, matplotlib, holidays, faker, seaborn, snowflake-connector-python, snowflake-snowpark-python\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "Successfully installed asn1crypto-1.5.1 cloudpickle-2.0.0 contourpy-1.3.0 cycler-0.12.1 faker-30.1.0 filelock-3.16.1 fonttools-4.54.1 holidays-0.57 importlib-resources-6.4.5 joblib-1.4.2 kiwisolver-1.4.7 matplotlib-3.9.2 nvidia-nccl-cu12-2.23.4 pillow-10.4.0 pyarrow-17.0.0 pyjwt-2.9.0 pyparsing-3.1.4 scikit-learn-1.5.2 scipy-1.13.1 seaborn-0.13.2 snowflake-connector-python-3.12.2 snowflake-snowpark-python-1.9.0 sortedcontainers-2.4.0 threadpoolctl-3.5.0 tomlkit-0.13.2 urllib3-1.26.20 xgboost-2.1.1\n",
      "Found existing installation: urllib3 1.26.20\n",
      "Uninstalling urllib3-1.26.20:\n",
      "  Successfully uninstalled urllib3-1.26.20\n",
      "Collecting urllib3==1.26.15\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Installing collected packages: urllib3\n",
      "Successfully installed urllib3-1.26.15\n",
      "Collecting fosforml==1.1.6\n",
      "  Downloading fosforml-1.1.6-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cloudpickle==2.2.1 (from fosforml==1.1.6)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting scikit-learn==1.3.2 (from fosforml==1.1.6)\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting snowflake-ml-python==1.5.0 (from fosforml==1.1.6)\n",
      "  Downloading snowflake_ml_python-1.5.0-py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from scikit-learn==1.3.2->fosforml==1.1.6) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn==1.3.2->fosforml==1.1.6) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn==1.3.2->fosforml==1.1.6) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn==1.3.2->fosforml==1.1.6) (3.5.0)\n",
      "Collecting absl-py<2,>=0.15 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting cachetools<6,>=3.1.1 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting catboost<1.3,>=1.2.0 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading catboost-1.2.7-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting fsspec<2024,>=2022.11 (from fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: importlib-resources<7,>=6.1.1 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0->fosforml==1.1.6) (6.4.5)\n",
      "Collecting packaging<24,>=20.9 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas<3,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0->fosforml==1.1.6) (2.2.0)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0->fosforml==1.1.6) (17.0.0)\n",
      "Collecting pytimeparse<2,>=1.1.8 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading pytimeparse-1.1.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0->fosforml==1.1.6) (6.0.1)\n",
      "Collecting retrying<2,>=1.3.3 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting s3fs<2024,>=2022.11 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: snowflake-connector-python<4,>=3.5.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (3.12.2)\n",
      "Collecting snowflake-snowpark-python!=1.12.0,<2,>=1.11.1 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading snowflake_snowpark_python-1.22.1-py3-none-any.whl.metadata (94 kB)\n",
      "Collecting sqlparse<1,>=0.4 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0->fosforml==1.1.6) (4.12.2)\n",
      "Collecting xgboost<2,>=1.7.3 (from snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.9/site-packages (from anyio<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.9/site-packages (from anyio<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.9/site-packages (from anyio<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.2.2)\n",
      "Collecting graphviz (from catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (3.9.2)\n",
      "Collecting plotly (from catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2.32.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading aiohttp-3.10.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources<7,>=6.1.1->snowflake-ml-python==1.5.0->fosforml==1.1.6) (3.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas<3,>=1.0.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas<3,>=1.0.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.9/site-packages (from pandas<3,>=1.0.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2024.1)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading aiobotocore-2.15.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (42.0.5)\n",
      "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (24.0.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2024.7.4)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (3.16.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (4.2.2)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (0.13.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.26.15)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python!=1.12.0,<2,>=1.11.1->snowflake-ml-python==1.5.0->fosforml==1.1.6) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python!=1.12.0,<2,>=1.11.1->snowflake-ml-python==1.5.0->fosforml==1.1.6) (0.37.1)\n",
      "Collecting botocore<1.35.24,>=1.35.16 (from aiobotocore<3.0.0,>=2.5.4->s3fs<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading botocore-1.35.23-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading yarl-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4,>=3.5.0->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (2.21)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.9/site-packages (from matplotlib->catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6) (3.1.4)\n",
      "Collecting tenacity>=6.2.0 (from plotly->catboost<1.3,>=1.2.0->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.35.24,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs<2024,>=2022.11->snowflake-ml-python==1.5.0->fosforml==1.1.6)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading fosforml-1.1.6-py3-none-any.whl (40 kB)\n",
      "Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m166.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading snowflake_ml_python-1.5.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading catboost-1.2.7-cp39-cp39-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m198.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading pytimeparse-1.1.8-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading s3fs-2023.12.2-py3-none-any.whl (28 kB)\n",
      "Downloading snowflake_snowpark_python-1.22.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiobotocore-2.15.1-py3-none-any.whl (77 kB)\n",
      "Downloading aiohttp-3.10.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Downloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading botocore-1.35.23-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Downloading yarl-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: pytimeparse, wrapt, tenacity, sqlparse, retrying, packaging, multidict, jmespath, graphviz, fsspec, frozenlist, cloudpickle, cachetools, async-timeout, anyio, aioitertools, aiohappyeyeballs, absl-py, yarl, xgboost, scikit-learn, plotly, botocore, aiosignal, catboost, aiohttp, aiobotocore, snowflake-snowpark-python, s3fs, snowflake-ml-python, fosforml\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.4.0\n",
      "    Uninstalling anyio-4.4.0:\n",
      "      Successfully uninstalled anyio-4.4.0\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 2.1.1\n",
      "    Uninstalling xgboost-2.1.1:\n",
      "      Successfully uninstalled xgboost-2.1.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "  Attempting uninstall: snowflake-snowpark-python\n",
      "    Found existing installation: snowflake-snowpark-python 1.9.0\n",
      "    Uninstalling snowflake-snowpark-python-1.9.0:\n",
      "      Successfully uninstalled snowflake-snowpark-python-1.9.0\n",
      "Successfully installed absl-py-1.4.0 aiobotocore-2.15.1 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aioitertools-0.12.0 aiosignal-1.3.1 anyio-3.7.1 async-timeout-4.0.3 botocore-1.35.23 cachetools-5.5.0 catboost-1.2.7 cloudpickle-2.2.1 fosforml-1.1.6 frozenlist-1.4.1 fsspec-2023.12.2 graphviz-0.20.3 jmespath-1.0.1 multidict-6.1.0 packaging-23.2 plotly-5.24.1 pytimeparse-1.1.8 retrying-1.3.4 s3fs-2023.12.2 scikit-learn-1.3.2 snowflake-ml-python-1.5.0 snowflake-snowpark-python-1.22.1 sqlparse-0.5.1 tenacity-9.0.0 wrapt-1.16.0 xgboost-1.7.6 yarl-1.13.1\n",
      "Collecting yellowbrick\n",
      "  Downloading yellowbrick-1.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from yellowbrick) (3.9.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from yellowbrick) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from yellowbrick) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.9/site-packages (from yellowbrick) (1.26.4)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from yellowbrick) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (6.4.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0.0->yellowbrick) (3.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Downloading yellowbrick-1.5-py3-none-any.whl (282 kB)\n",
      "Installing collected packages: yellowbrick\n",
      "Successfully installed yellowbrick-1.5\n"
     ]
    }
   ],
   "source": [
    "# Upgrade pip to the latest version to avoid compatibility issues\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install Snowflake connectors along with Pandas integration for Snowflake, \n",
    "# as well as necessary libraries for data processing and machine learning (e.g., numpy, scikit-learn, xgboost, matplotlib, etc.)\n",
    "!pip install \"snowflake-connector-python[pandas]\" \"snowflake-snowpark-python[pandas]\" snowflake-snowpark-python==1.9.0 numpy pandas matplotlib scikit-learn xgboost seaborn python-dateutil tqdm holidays faker\n",
    "\n",
    "# Ensure Snowflake Snowpark Python is upgraded to the desired version (1.9.0)\n",
    "!pip install --upgrade --q snowflake-snowpark-python==1.9.0\n",
    "\n",
    "# Uninstalling the current version of urllib3 to avoid version conflicts\n",
    "!pip uninstall urllib3 -y\n",
    "\n",
    "# Installing a specific version of urllib3 (1.26.15) known to work with Snowflake and other libraries\n",
    "!pip install urllib3==1.26.15\n",
    "\n",
    "# Install fosforml library for machine learning model management and integration\n",
    "!pip install fosforml==1.1.6\n",
    "\n",
    "# Install Yellowbrick for machine learning visualization\n",
    "!pip install yellowbrick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485aa556-2a7a-49db-98e6-0a5787b2472f",
   "metadata": {},
   "source": [
    "# Importing Required Libraries for Data Processing, Visualization, and Modeling\n",
    "\n",
    "### In this section, we import various libraries needed for data manipulation, visualization, machine learning, and system utilities. These libraries will help in tasks such as configuring the environment, plotting graphs, managing models, and performing advanced mathematical operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2108f41f-a786-4e1a-814c-fbe016a84e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules from the fosforml library for model management\n",
    "from fosforml import *\n",
    "\n",
    "# Import model flavor constants from fosforml for handling various ML model types\n",
    "from fosforml.constants import MLModelFlavours\n",
    "\n",
    "# Importing visualization library (matplotlib) for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import essential libraries for data manipulation and numerical operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Display a larger number of columns in the DataFrame for better visibility\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Seaborn for statistical data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn's metric for calculating Mean Absolute Percentage Error (MAPE)\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Suppress warnings for cleaner output in the notebook\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# Joblib for saving and loading ML models efficiently\n",
    "from joblib import dump, load\n",
    "\n",
    "# Requests for making HTTP requests to access data from external sources\n",
    "import requests\n",
    "\n",
    "# tqdm for progress bars, especially for loops that take a long time to execute\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Time utilities for time-based operations\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "# Sleep function to add delays in execution, useful for timing-based operations\n",
    "from time import sleep\n",
    "\n",
    "# ConfigParser for reading configuration files\n",
    "import configparser\n",
    "\n",
    "# Date manipulation library for working with date ranges, intervals, and easter date calculations\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from dateutil.easter import easter\n",
    "\n",
    "# Import functions from scipy for mathematical optimizations and curve fitting\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Enable inline plotting for Matplotlib, ensuring plots appear within the notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958a442-ccf3-47d3-a04c-dd7c506fc6a0",
   "metadata": {},
   "source": [
    "# Configuring Matplotlib and Suppressing Warnings\n",
    "\n",
    "### This section configures Matplotlib for consistent visualization aesthetics and suppresses warnings to ensure a clean output in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4178e26a-da98-4398-876f-d4365aab4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Matplotlib's default font family to 'DeJavu Serif' to ensure a consistent font style across plots\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "\n",
    "# Import the warnings library to suppress unnecessary warnings\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings for cleaner notebook output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing rcParams from Matplotlib for further font configuration\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Configure Matplotlib to use 'DejaVu Sans' font to avoid 'sans-serif' related warnings\n",
    "rcParams['font.family'] = 'DejaVu Sans'  # or another system-available font\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85cf6d-977c-4198-a366-d6e50b4c5f61",
   "metadata": {},
   "source": [
    "# Importing the Required Libraries for Data Processing, Visualization, and Clustering\n",
    "\n",
    "### In this section, we import libraries for handling various tasks such as data preprocessing, visualization, and applying machine learning clustering algorithms. Warnings are also suppressed for cleaner outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac06fe0-d1f4-4293-9752-c51ee59c69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing datetime to work with date and time objects\n",
    "import datetime\n",
    "\n",
    "# Importing Matplotlib and related components for data visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# Seaborn for statistical data visualizations\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing Scikit-learn's preprocessing utilities for encoding categorical variables and scaling numerical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PCA (Principal Component Analysis) for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Yellowbrick's KElbowVisualizer to help determine the optimal number of clusters for KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# KMeans clustering algorithm from Scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Importing 3D plotting support from Matplotlib for visualizing clusters in 3D space\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Agglomerative Clustering algorithm for hierarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Importing Matplotlib colormap utilities to handle color mapping for plots\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Scikit-learn's metrics module for evaluating clustering performance\n",
    "from sklearn import metrics\n",
    "\n",
    "# Importing warnings library to suppress any unnecessary warnings during execution\n",
    "import warnings\n",
    "\n",
    "# Import sys to check and apply warnings filter conditions\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    # Suppressing all warnings to clean up output\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Set a random seed for reproducibility of results\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc1224-af2f-42d0-8d3c-75f5d7f79e43",
   "metadata": {},
   "source": [
    "# Establishing a Snowflake Session for Data Operations\n",
    "\n",
    "### In this section, we establish a connection to the Snowflake database using the `fosforml` library. This session will be used to execute queries and perform data operations within Snowflake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6d7cf3-d0f7-4c85-aaf2-569790eb8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the get_session function from fosforml's Snowflake session manager\n",
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "# Establishing a Snowflake session for executing queries and performing operations\n",
    "my_session = get_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b3c58-c8b0-4b74-b8bc-66c770a2ae0c",
   "metadata": {},
   "source": [
    "# Loading Customer Data into a Pandas DataFrame\n",
    "\n",
    "### In this section, we load customer data from a CSV file into a Pandas DataFrame. This dataset will be used for further analysis and processing in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a14dfc-0407-48af-8678-20f0a6380c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the customer data from a CSV file into a Pandas DataFrame\n",
    "cust_df = pd.read_csv('customer_table.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect the loaded data\n",
    "cust_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f7a5c-75e3-4744-8085-e71eb1e02589",
   "metadata": {},
   "source": [
    "# Loading Transaction Data from CSV Files\n",
    "\n",
    "### Transaction data from multiple CSV files is loaded into separate Pandas DataFrames. These datasets will be merged for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47127e9e-a3b7-4ef8-a6cb-155c08f4445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data from multiple CSV files into separate DataFrames\n",
    "t1_df = pd.read_csv('trx_1.csv')  # First transaction dataset\n",
    "t2_df = pd.read_csv('trx_2.csv')  # Second transaction dataset\n",
    "t3_df = pd.read_csv('trx_3.csv')  # Third transaction dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d59129-7410-4a78-8f1d-543e5eced180",
   "metadata": {},
   "source": [
    "# Writing Customer Data to Snowflake Table\n",
    "\n",
    "### The customer data is converted from a Pandas DataFrame into a Snowflake DataFrame and written to a Snowflake table named `casino_customers`. The `overwrite` mode ensures that the table is replaced if it already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd1834-972a-4e05-9c9f-2dcb331c4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame (cust_df) into a Snowflake DataFrame\n",
    "cust_sfdf = my_session.createDataFrame(cust_df)\n",
    "\n",
    "# Write the Snowflake DataFrame to a Snowflake table named 'casino_customers'\n",
    "# The 'overwrite' mode ensures that the table is replaced if it already exists\n",
    "cust_sfdf.write.mode(\"overwrite\").save_as_table(\"casino_customers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93de87-0839-4cb5-8bad-5c31e81b6215",
   "metadata": {},
   "source": [
    "# Merging Transaction Data into a Single DataFrame\n",
    "\n",
    "### The transaction data from multiple DataFrames is merged into a single DataFrame. This combined dataset will be used for further analysis. The `ignore_index=True` ensures that the index is reset after merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd48d9-b584-4f41-9a16-aaae3f327c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the first two transaction DataFrames (t1_df and t2_df) into one DataFrame\n",
    "inter_df = t1_df._append(t2_df, ignore_index=True)\n",
    "\n",
    "# Append the third transaction DataFrame (t3_df) to the intermediate DataFrame\n",
    "trx_df = inter_df._append(t3_df, ignore_index=True)\n",
    "\n",
    "# Display the structure and details of the combined transaction DataFrame\n",
    "trx_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b440b-8c89-4721-bcaa-6db4342ed004",
   "metadata": {},
   "source": [
    "# Writing Merged Transaction Data to Snowflake Table\n",
    "\n",
    "### The merged transaction data is converted into a Snowflake DataFrame and written to a Snowflake table named `casino_transactions`. The `overwrite` mode ensures that the table is replaced if it already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068b4b9-4f5e-4ec1-8d79-8ea4c148d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the merged Pandas DataFrame (trx_df) into a Snowflake DataFrame\n",
    "trx_sfdf = my_session.createDataFrame(trx_df)\n",
    "\n",
    "# Write the Snowflake DataFrame to a Snowflake table named 'casino_transactions'\n",
    "# The 'overwrite' mode ensures that the table is replaced if it already exists\n",
    "trx_sfdf.write.mode(\"overwrite\").save_as_table(\"casino_transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164904c6-1651-4a6c-a724-0eb2530c2cd6",
   "metadata": {},
   "source": [
    "# Querying Data from Snowflake and Converting to Pandas DataFrame\n",
    "\n",
    "### This section retrieves all records from the `CASINO_TRANSACTIONS` table in Snowflake and converts the resulting Snowflake DataFrame into a Pandas DataFrame for local analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1006648-392d-46b4-a60d-9057d304b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the Snowflake table to query\n",
    "table_name = 'CASINO_TRANSACTIONS'\n",
    "\n",
    "# Execute a SQL query to select all records from the specified table in Snowflake\n",
    "transaction_df = my_session.sql(\"select * from {}\".format(table_name))\n",
    "\n",
    "# Convert the Snowflake DataFrame to a Pandas DataFrame for further processing\n",
    "transaction_df = transaction_df.to_pandas()\n",
    "\n",
    "# Check the type of the resulting DataFrame to confirm it is a Pandas DataFrame\n",
    "type(transaction_df)\n",
    "\n",
    "\n",
    "transaction_df = trx_df.copy()\n",
    "transaction_df.columns = [col.upper() for col in transaction_df.columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8beb5ac-91f0-4c84-b467-460e34cf032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_aggregation = transaction_df.groupby('PLAYER_ID').agg(\n",
    "    DATE_FIRST_VISIT=('DATE', 'min'),\n",
    "    DATE_LAST_VISIT= ('DATE', 'max'),\n",
    "    TOTAL_NUMBER_OF_VISITS=('TRANSACTION_ID', 'count'),\n",
    "    TOTAL_DURATION_SPENT=('DURATION_SPENT', 'sum'),\n",
    "    AVERAGE_DURATION_PER_VISIT=('DURATION_SPENT', 'mean'),\n",
    "    TOTAL_CHIPS_WON_OR_LOST=('CHIPS_WON_OR_LOST', 'sum'),\n",
    "    AVERAGE_CHIPS_WON_OR_LOST_PER_VISIT=('CHIPS_WON_OR_LOST', 'mean'),\n",
    "    UNIQUE_GAMES_PLAYED=('GAME_NAME', 'nunique'),\n",
    "    IS_PREMIUM_PLAYER=('IS_PREMIUM_PLAYER', 'max'),\n",
    "    IS_LOYALTY_CARD_HOLDER=('IS_LOYALTY_CARD_HOLDER', 'max'),\n",
    "    TOTAL_AMOUNT_SPENT_IN_HOTEL=('AMOUNT_SPENT_IN_HOTEL_STAY', 'sum'),\n",
    "    TOTAL_DAYS_SPENT_HOTEL=('NUMBER_OF_DAYS_SPENT_IN_HOTEL', 'sum'),\n",
    "    TOTAL_AMOUNT_SPENT_IN_CASINO_RESTAURANT=('AMOUNT_SPENT_IN_CASINO_RESTAURANT', 'sum'),\n",
    "    NUMBER_OF_RESTAURANT_VISITS=('NUMBER_OF_RESTAURANT_VISITS', 'sum'),\n",
    "    TOTAL_AMOUNT_SPENT_IN_SPA=('AMOUNT_SPENT_IN_SPA', 'sum'),\n",
    "    NUMBER_OF_SPA_VISITS=('NUMBER_OF_SPA_VISITS', 'sum'),\n",
    "    TOTAL_REVENUE_TO_CASINO=('REVENUE_MADE_BY_CASINO_FROM_PLAYER', 'sum'),\n",
    "    NUMBER_OF_CONCIERGE_VISITS=('NUMBER_OF_CONCIERGE_VISITS', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eda159-6c0b-4004-8ad3-224078108361",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_aggregation['DATE_FIRST_VISIT'] = pd.to_datetime(customer_aggregation['DATE_FIRST_VISIT'], format = 'mixed')\n",
    "customer_aggregation['DATE_LAST_VISIT'] = pd.to_datetime(customer_aggregation['DATE_LAST_VISIT'], format = 'mixed')\n",
    "transaction_df['DATE'] = pd.to_datetime(transaction_df['DATE'], format = 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09affc80-a522-46c6-bd71-b635b0af0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_aggregation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e4c3d-94fe-4f7b-bc0f-3e234f6b4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_aggregation[customer_aggregation['DATE_FIRST_VISIT']<customer_aggregation['DATE_LAST_VISIT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b579f-dfce-4424-a35a-254d9743def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating preferred game category and name\n",
    "preferred_game = transaction_df.groupby(['PLAYER_ID', 'GAME_CATEGORY', 'GAME_NAME','PLAYER_AGE', 'PLAYER_GENDER', 'HOME_COUNTRY', 'HOME_CITY'])['DURATION_SPENT'].sum().reset_index()\n",
    "preferred_game = preferred_game.loc[preferred_game.groupby('PLAYER_ID')['DURATION_SPENT'].idxmax()][['PLAYER_ID', 'GAME_CATEGORY', 'GAME_NAME', 'PLAYER_AGE', 'PLAYER_GENDER', 'HOME_COUNTRY', 'HOME_CITY']]\n",
    "preferred_game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3705e-0372-4955-8580-ce6a76e6cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_aggregation = customer_aggregation.merge(preferred_game, on='PLAYER_ID', how='left')\n",
    "customer_aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d05d5-89ce-4907-afe8-4cf90878a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_aggregation.rename(columns= {'PLAYER_AGE':'AGE' , 'PLAYER_GENDER':'GENDER', 'GAME_CATEGORY':'PREFERRED_GAME_CATEGORY', \n",
    "                                      'GAME_NAME':'PREFERRED_GAME_NAME'}, inplace= True) \n",
    "customer_aggregation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96f8bb-1134-46aa-9a1d-97216227e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = customer_aggregation.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031f34a-e19d-46de-8017-ee7c1435c43b",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35234a2-426d-465a-a020-68e400322bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date transformation data type\n",
    "\n",
    "customer_df['DATE_FIRST_VISIT'] = pd.to_datetime(customer_df['DATE_FIRST_VISIT'], format = 'mixed')\n",
    "customer_df['DATE_LAST_VISIT'] = pd.to_datetime(customer_df['DATE_LAST_VISIT'], format = 'mixed')\n",
    "transaction_df['DATE'] = pd.to_datetime(transaction_df['DATE'], format = 'mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5ca00-3f88-45ee-bc94-3b444340d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customer_df.info())\n",
    "print(transaction_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeab952-c297-4f07-a468-16278df82f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new features for better classification\n",
    "customer_df['VISIT_FREQUENCY'] = customer_df['TOTAL_NUMBER_OF_VISITS'] / ((pd.to_datetime(customer_df['DATE_LAST_VISIT']) - \n",
    "                                                                           pd.to_datetime(customer_df['DATE_FIRST_VISIT'])).dt.days + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1876449-17b6-4bdb-be07-bd9f5613d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of categorical variables\n",
    "s = (customer_df.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables in the dataset:\", object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc52b1f-b8b5-4026-93d3-5fb40b708a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding the object dtypes.\n",
    "LE=LabelEncoder()\n",
    "for i in object_cols:\n",
    "    customer_df[i]=customer_df[[i]].apply(LE.fit_transform)\n",
    "    \n",
    "print(\"All features are now numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dcc897-782c-4e50-b933-8e2f8b8cd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f30f1-9d67-464d-a826-a885cf311489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customer_df.max(axis=0)) # will return max value of each column\n",
    "print(customer_df.min(axis=0)) # will return min value of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc7de7-85ce-4fcb-aad0-492769bd02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customer_df.mean(axis=0)) # will return min value of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0c55a-d13a-4f33-8797-96a75044f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf_df = my_session.createDataFrame(customer_df)\n",
    "# sf_df.write.mode(\"overwrite\").save_as_table(\"CASINO_CUSTOMERS\")\n",
    "# my_session.table(\"CASINO_CUSTOMERS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a73fd-8cfa-44e6-ad75-d7431736c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e9931-f6db-4939-9f0b-f2ae7af13506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of data\n",
    "ds = customer_df.copy()\n",
    "\n",
    "# Creating a subset of dataframe by dropping certain columns\n",
    "cols_del = ['DATE_FIRST_VISIT', 'DATE_LAST_VISIT']\n",
    "ds = ds.drop(cols_del, axis=1)\n",
    "\n",
    "# Check for NaN, inf, or -inf in the dataset\n",
    "print(f\"NaN values in the dataset:\\n{ds.isna().sum()}\")\n",
    "print(f\"Any inf or -inf in the dataset:\\n{(ds == np.inf).sum() + (ds == -np.inf).sum()}\")\n",
    "\n",
    "# Option 1: Replace inf, -inf, and NaN with a specified value (e.g., mean, median, or 0)\n",
    "ds = ds.replace([np.inf, -np.inf], np.nan)  # Replace inf/-inf with NaN\n",
    "ds.fillna(ds.mean(), inplace=True)  # Replace NaN with mean of each column\n",
    "\n",
    "# Option 2: Alternatively, you can drop rows containing these values\n",
    "# ds = ds.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(ds)\n",
    "scaled_ds = pd.DataFrame(scaler.transform(ds), columns=ds.columns)\n",
    "\n",
    "print(\"All features are now scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1c766-135f-485b-a5b9-ccc60577d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e779c4-ecab-48e2-a20e-4e7d1e395d78",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1eb6e-1a91-43a1-9120-5a25a8f1ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for customer data\n",
    "print(customer_df.describe(include='all'))\n",
    "print(transaction_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a6448-f997-4f88-8d48-daa3a36cd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of age\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Configure the font family to avoid 'sans-serif' warnings\n",
    "rcParams['font.family'] = 'DejaVu Sans'  # or another available font on your system\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(customer_df['AGE'], bins=20, kde=True)\n",
    "plt.title('Age Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e8e92-2a55-4b5e-b4eb-361128547799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(customer_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429f8ee-7057-4153-bddd-faaa3ead95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values or drop them if appropriate\n",
    "customer_df = customer_df.dropna()  # Here we simply drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c8c24-fdfd-4b28-a7b5-712150e60ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a copy of the dataset\n",
    "ds = customer_df.copy()\n",
    "\n",
    "# Check for and handle 'inf' or '-inf' values\n",
    "ds.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Option 1: Drop rows containing NaN values\n",
    "ds.dropna(inplace=True)\n",
    "\n",
    "# Option 2: Alternatively, you can fill NaN values with the mean or median\n",
    "# ds.fillna(ds.mean(), inplace=True)\n",
    "\n",
    "# Plotting distribution for numeric features in the customer data\n",
    "ds.hist(bins=15, figsize=(20, 15))\n",
    "plt.suptitle('Distribution of Numeric Features in Customer Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78bf02-3689-409e-b30d-4d0c23d12ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Correlation matrix for customer_df\n",
    "correlation_matrix = customer_df.corr()\n",
    "\n",
    "# Set the figure size larger\n",
    "plt.figure(figsize=(25, 20))\n",
    "\n",
    "# Create the heatmap with larger annotations\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    annot_kws={\"size\": 12},  # Increase the annotation font size\n",
    "    linewidths=0.5,          # Add spacing between cells for better readability\n",
    "    linecolor='gray'         # Add lines to separate cells\n",
    ")\n",
    "\n",
    "# Add title and display\n",
    "plt.title('Correlation Matrix for Customer Data', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47919555-52fd-404f-9a65-9043ced54a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat= customer_df.corr()\n",
    "plt.figure(figsize=(20,20))  \n",
    "sns.heatmap(corrmat,annot=True, cmap='coolwarm', center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda10d2-dbb6-478d-85aa-52a7cea4ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f962d-7dd1-4520-8245-78338aec5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3febbf7-b1fb-433a-bd1d-5c00f16c35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting categorical features against target variable\n",
    "sns.countplot(x='GENDER', data=customer_df)\n",
    "plt.title('Gender Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9670e-c681-491e-aa3d-4069804edca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='HOME_COUNTRY', data=customer_df)\n",
    "plt.title('Country Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b31dd-de66-465a-a09d-0335010d69d9",
   "metadata": {},
   "source": [
    "# PCA: Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2dc35d-5f05-4dd0-9bee-f123509c9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating PCA to reduce dimentions aka features to 3\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(scaled_ds)\n",
    "PCA_ds = pd.DataFrame(pca.transform(scaled_ds), columns=([\"col1\",\"col2\", \"col3\"]))\n",
    "PCA_ds.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54318f-040c-431d-b1dd-50bb4f8451ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A 3D Projection Of Data In The Reduced Dimension\n",
    "x =PCA_ds[\"col1\"]\n",
    "y =PCA_ds[\"col2\"]\n",
    "z =PCA_ds[\"col3\"]\n",
    "#To plot\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(x,y,z, c=\"blue\", marker=\"o\" )\n",
    "ax.set_title(\"A 3D Projection Of Data In The Reduced Dimension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d24ebd-09f5-4449-b432-5fd8172fe086",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e07ce-e235-46ff-84aa-92140e8005fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick examination of elbow method to find numbers of clusters to make.\n",
    "print('Elbow Method to determine the number of clusters to be formed:')\n",
    "Elbow_M = KElbowVisualizer(KMeans(), k=10)\n",
    "Elbow_M.fit(PCA_ds)\n",
    "Elbow_M.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291ede3-e798-4e5d-978a-0b0f9975d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick examination of elbow method to find numbers of clusters to make.\n",
    "print('Elbow Method to determine the number of clusters to be formed:')\n",
    "Elbow_M = KElbowVisualizer(KMeans(), k=10)\n",
    "Elbow_M.fit(scaled_ds)\n",
    "Elbow_M.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0129f-fc2a-4441-b28b-5436dd19be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the Agglomerative Clustering model \n",
    "AC = AgglomerativeClustering(n_clusters=4)\n",
    "\n",
    "# fit model and predict clusters\n",
    "yhat_AC = AC.fit_predict(PCA_ds)\n",
    "PCA_ds[\"CLUSTERS\"] = yhat_AC\n",
    "\n",
    "#Adding the Clusters feature to the orignal dataframe.\n",
    "customer_df[\"CLUSTERS\"]= yhat_AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853961d1-b55a-4398-8927-39ffa009a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the clusters\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "\n",
    "# Use a valid colormap like 'viridis'\n",
    "ax.scatter(x, y, z, s=40, c=PCA_ds[\"CLUSTERS\"], marker='o', cmap='viridis')\n",
    "\n",
    "# Set the title\n",
    "ax.set_title(\"The Plot Of The Clusters\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38278e9-ba55-402f-bc42-e9b596888b6a",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206acfef-f070-40c9-bdbc-b1c92d9c8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting countplot of clusters\n",
    "pal = [\"#FF5733\",\"#33FF57\", \"#3357FF\",\"#FF33A1\",] #\"#FFD700\"]\n",
    "pl = sns.countplot(x=customer_df[\"CLUSTERS\"], palette= pal)\n",
    "pl.set_title(\"Distribution Of The Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1ad4e-c471-4ff4-91f7-0abc24e2e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = sns.scatterplot(data = customer_df,x=customer_df[\"TOTAL_REVENUE_TO_CASINO\"], y=customer_df[\"VISIT_FREQUENCY\"],hue=customer_df[\"CLUSTERS\"], palette= pal)\n",
    "pl.set_title(\"Cluster's Profile Based On revenue to Casino\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83cb1a-cffc-4c48-9273-a8692625fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pl=sns.swarmplot(x=customer_df[\"CLUSTERS\"], y=customer_df[\"TOTAL_REVENUE_TO_CASINO\"], color= \"#CBEDDD\", alpha=0.5 )\n",
    "pl=sns.boxenplot(x=customer_df[\"CLUSTERS\"], y=customer_df[\"TOTAL_REVENUE_TO_CASINO\"], palette=pal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61148d0-2b23-442d-a10f-2a801c685370",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pl=sns.swarmplot(x=customer_df[\"CLUSTERS\"], y=customer_df[\"TOTAL_NUMBER_OF_VISITS\"], color= \"#CBEDDD\", alpha=0.5 )\n",
    "pl=sns.boxenplot(x=customer_df[\"CLUSTERS\"], y=customer_df[\"TOTAL_NUMBER_OF_VISITS\"], palette=pal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fb11f-3c78-49b6-8b99-beedc4501fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "plt.figure()\n",
    "pl=sns.boxenplot(y=customer_df[\"TOTAL_CHIPS_WON_OR_LOST\"],x=customer_df[\"CLUSTERS\"], palette= pal)\n",
    "pl.set_title(\"TOTAL_CHIPS_WON_OR_LOST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d486c60-4845-4301-80e8-0c25844ddd17",
   "metadata": {},
   "source": [
    "# PROFILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b867a3e-706a-450e-9861-18e8ece1f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [ \"AGE\", \"GENDER\",\"HOME_COUNTRY\",\"TOTAL_DURATION_SPENT\",\"TOTAL_CHIPS_WON_OR_LOST\",\"AVERAGE_DURATION_PER_VISIT\",\n",
    "               \"AVERAGE_CHIPS_WON_OR_LOST_PER_VISIT\", \"UNIQUE_GAMES_PLAYED\", \n",
    "               \"IS_PREMIUM_PLAYER\", \"IS_LOYALTY_CARD_HOLDER\", \"TOTAL_AMOUNT_SPENT_IN_HOTEL\", \"TOTAL_DAYS_SPENT_HOTEL\",\n",
    "               \"TOTAL_AMOUNT_SPENT_IN_CASINO_RESTAURANT\",\"NUMBER_OF_RESTAURANT_VISITS\", \"TOTAL_AMOUNT_SPENT_IN_SPA\", \"NUMBER_OF_SPA_VISITS\",\n",
    "               \"TOTAL_REVENUE_TO_CASINO\", \"NUMBER_OF_CONCIERGE_VISITS\", \"VISIT_FREQUENCY\"]\n",
    "\n",
    "for i in column_list:\n",
    "    plt.figure()\n",
    "    sns.jointplot(x=customer_df[i], y=customer_df[\"TOTAL_REVENUE_TO_CASINO\"], hue =customer_df[\"CLUSTERS\"], kind=\"kde\", palette=pal)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242118d-af4d-4d24-b455-43c43d599557",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852bd71-cc47-419d-8926-9332ac41c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_mapping = {\n",
    "    0: 'High roller Professionals',\n",
    "    1: 'Conservative Low spenders',\n",
    "    2: 'Mediocre cross spending players',\n",
    "    3: 'Money losing players'\n",
    "}\n",
    "\n",
    "# Use the map function to assign player segments based on the cluster labels\n",
    "customer_df['PLAYER_SEGMENT'] = customer_df['CLUSTERS'].map(segment_mapping)\n",
    "customer_df['PLAYER_SEGMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458c0e2-7a4e-4084-8d9e-b8eb0cca2de3",
   "metadata": {},
   "source": [
    "# Assigning Detailed Segment Descriptions Based on Cluster Labels\n",
    "\n",
    "### A more efficient approach to assigning detailed descriptions to each customer segment based on cluster membership using a dictionary mapping. This method improves code readability and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da5a88-780d-4338-88c6-e5d5e40da046",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_desc_mapping = {\n",
    "    0: 'Risk taking professional, regular players with deep pockets spending across services',\n",
    "    1: 'Low spends, low money making low risk players',\n",
    "    2: 'Budding good Players with potential to win, exploring multiple casino services',\n",
    "    3: 'Money losing players'\n",
    "}\n",
    "\n",
    "# Use the map function to assign segment descriptions based on the cluster labels\n",
    "customer_df['SEGMENT_DESC'] = customer_df['CLUSTERS'].map(segment_desc_mapping)\n",
    "customer_df['SEGMENT_DESC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6aef1-c10c-4db5-82b7-92be4bc5d951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7000f94d-8cd7-4a83-98d4-21ab27e3f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session.table(['CUSTOMER_SEGMENTS_LATEST']).to_pandas().to_csv('CUSTOMER_SEGMENTS_LATEST.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585d9be-c61f-4057-aed5-037dfd861b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
