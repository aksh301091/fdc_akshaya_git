{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0e4ded",
   "metadata": {},
   "source": [
    "# Price Optimization for Hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fb829",
   "metadata": {},
   "source": [
    "# Getting snowflake related packages installed and imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222af8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting snowflake-snowpark-python==1.9.0\n",
      "  Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fosforio\n",
      "  Downloading fosforio-1.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fosforml\n",
      "  Downloading fosforml-1.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting python-dateutil\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting holidays\n",
      "  Downloading holidays-0.51-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading snowflake_connector_python-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m253.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setuptools>=40.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading setuptools-70.1.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting wheel (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.1.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyyaml (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cffi<2.0.0,>=1.9 (from snowflake-connector-python[pandas])\n",
      "  Downloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting cryptography<43.0.0,>=3.1.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading cryptography-42.0.8-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting pyOpenSSL<25.0.0,>=16.2.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading pyOpenSSL-24.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyjwt<3.0.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pytz (from snowflake-connector-python[pandas])\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting requests<3.0.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting packaging (from snowflake-connector-python[pandas])\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from snowflake-connector-python[pandas])\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from snowflake-connector-python[pandas])\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting platformdirs<5.0.0,>=2.6.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tomlkit (from snowflake-connector-python[pandas])\n",
      "  Downloading tomlkit-0.12.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting urllib3<2.0.0,>=1.21.1 (from snowflake-connector-python[pandas])\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m227.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow (from snowflake-connector-python[pandas])\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "INFO: pip is looking at multiple versions of fosforml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fosforml\n",
      "  Downloading fosforml-1.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting requests-toolbelt==0.9.1 (from fosforml)\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting shutils==0.1.0 (from fosforml)\n",
      "  Downloading shutils-0.1.0.tar.gz (2.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting mosaic-utils (from fosforml)\n",
      "  Downloading mosaic_utils-1.0.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<2.0.0,>=1.21.1 (from snowflake-connector-python[pandas])\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m239.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting configparser (from shutils==0.1.0->fosforml)\n",
      "  Downloading configparser-7.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pymysql (from shutils==0.1.0->fosforml)\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading pillow-10.3.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m250.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting six>=1.5 (from python-dateutil)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pycparser (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas])\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib)\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fosforio-1.0.2-py3-none-any.whl (20 kB)\n",
      "Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m236.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fosforml-1.0.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m225.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m362.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m262.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m322.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m237.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m252.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m176.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m345.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m328.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m291.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading holidays-0.51-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m319.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m287.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m312.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.7/444.7 kB\u001b[0m \u001b[31m344.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m304.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m341.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-42.0.8-cp37-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m182.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading fonttools-4.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m188.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m258.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m336.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m315.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m251.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m187.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m273.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m268.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m350.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m271.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m177.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-70.1.1-py3-none-any.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.3/883.3 kB\u001b[0m \u001b[31m328.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading snowflake_connector_python-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m188.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m251.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mosaic_utils-1.0.2-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m313.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m181.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.9 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/40.9 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"snowflake-connector-python[pandas]\" \"snowflake-snowpark-python[pandas]\" snowflake-snowpark-python==1.9.0 fosforio fosforml numpy pandas matplotlib scikit-learn xgboost seaborn python-dateutil tqdm holidays\n",
    "!pip install --upgrade --q snowflake-snowpark-python==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab3f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c504d4e",
   "metadata": {},
   "source": [
    "# Restart kernal and continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14007d14",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforio import snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81490294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "import configparser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03df6c",
   "metadata": {},
   "source": [
    "# Getting data from snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307b6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get snowflake connection object with a default snowflake connection created by the user, if available.\n",
    "snowflake.get_connection(\"TTH_REV_OPT_CXN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ef799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = snowflake.get_dataframe(\"BOOKINGS_TRANSFORMED\")\n",
    "df['ADR']= round(df['ADR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download star schema tables\n",
    "meal = \n",
    "customer = \n",
    "channel = \n",
    "room = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [x.lower() for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98990657",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x=\"reserved_room_type\", y=\"adr\", hue= 'is_canceled', color=\".8\", linecolor=\"#137\", linewidth=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles_data = df.groupby(['hotel', 'reserved_room_type'])['adr'].agg(\n",
    "    adr_mean='mean',\n",
    "    adr_median='median',\n",
    "    adr_q01=lambda x: x.quantile(0.10),\n",
    "    adr_q1=lambda x: x.quantile(0.25),\n",
    "    adr_q2=lambda x: x.quantile(0.50),\n",
    "    adr_q3=lambda x: x.quantile(0.75),\n",
    "    adr_q4=lambda x: x.quantile(1.00)\n",
    ").reset_index()\n",
    "print(quartiles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the quartiles data with the original data\n",
    "merged_data = pd.merge(df, quartiles_data, on=['hotel', 'reserved_room_type'])\n",
    "\n",
    "print(\"\\nMerged Data:\")\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = merged_data[(merged_data['adr'] > merged_data['adr_q01']) & \n",
    "                            (merged_data['adr'] < merged_data['adr_q4'])]\n",
    "\n",
    "print(\"\\nFiltered Data:\")\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be354cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some column names\n",
    "filtered_data.rename(columns = {'arrival_date_transformed':'arrival_date', \n",
    "                                'reservation_status_date_transformed':'reservation_date',\n",
    "                                'arrival_date_year':'year','arrival_date_week_number': 'week', 'arrival_date_day_of_month': 'dom', \n",
    "                                'total_stay_nights': 'total_rns',\n",
    "                                'stays_in_weekend_nights':'weekend_nights', 'stays_in_week_nights': 'week_nights',\n",
    "                                'reserved_room_type': 'room_type', 'customer_type':'cust_type', 'ADR':'adr'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=filtered_data, x=\"room_type\", y=\"adr\", hue= 'is_canceled', color=\".8\", linecolor=\"#137\", linewidth=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=filtered_data, x=\"room_type\", y=\"adr\", hue= 'market_segment', \n",
    "            fill=False, gap=.1, linecolor=\"#137\", linewidth=.75, color= 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined column for the plot\n",
    "filtered_data['Group'] = filtered_data['hotel'] + ' - ' + filtered_data['room_type']\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Create boxplot\n",
    "sns.boxplot(x='Group', y='adr', data=filtered_data)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Boxplot of ADR by Hotel and Reserved Room Type')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('adr')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e978f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined column for the plot\n",
    "filtered_data['Group3'] = filtered_data['room_type'] + ' - ' + filtered_data['reservation_status']\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Create boxplot\n",
    "sns.boxplot(x='Group3', y='adr', data=filtered_data)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Boxplot of ADR by CANCELED and Reserved Room Type')\n",
    "plt.xlabel('Group3')\n",
    "plt.ylabel('adr')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218ea6",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf15b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing cancelled transactions to model based on transactions that customers preferred to actually complete\n",
    "df0 = filtered_data[(filtered_data['is_canceled'] == 0) & (filtered_data['reservation_status'] !='No-Show')] \n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99536bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding what market segments need to be considered\n",
    "df0.groupby(['hotel','room_type','market_segment']).agg({'adr':'mean','reservation_date':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8712a",
   "metadata": {},
   "source": [
    "#We will filter for room _types A,D,E and will filter OUT \"Complementary\" market_segment. retain both City and resort hotel types.\n",
    "##\"For our pricing model, we need to count the frequency of each price point\n",
    "\n",
    "##Moreover, Hotels have different room types with different prices. To create our pricing model, we need to optimize the room type level. We are interested in the room type reserved as this captures the price the guest was willing to pay for that room type. It is common for hotels to overbook the lower room types and give free upgrades to higher room types. Therefore, any pricing analysis must be done at the reserved room type and not the stayed room type.\n",
    "\n",
    "##Unfortunately, there is not much description of the reserved room type, but by looking at the price points, we can infer that A &B are the lowest price points. The rest of the room types command a higher ADR, which means they likely are upgraded rooms. In another workbook, we will explore how to price those premium rooms, but for the moment, let's focus on room types A & D, E.\n",
    "\n",
    "#filter data to only Direct and Online TA and room typ A&B\n",
    "##Create individual datasets for City, Resort, room types: A,D,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = df0[(df0['market_segment'] != 'Complementary') & (df0['market_segment'] != 'Corporate') \n",
    "#           & (df0['market_segment'] !='Aviation')]\n",
    "data1 = df0[~df0['market_segment'].isin(['Complementary', 'Corporate', 'Aviation'])]\n",
    "#data1 = df0[(df0['room_type'] == 'A') |(df0['room_type'] == 'D') | (df0['room_type'] == 'E')]\n",
    "data1.room_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf42531",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.market_segment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031841de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding what market segments need to be considered\n",
    "data1.groupby(['hotel','room_type','market_segment']).agg({'adr':'mean','reservation_date':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b66650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "data1.groupby(['arrival_date']).agg({'adr':'mean','reservation_date':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf489a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unrequired columns\n",
    "data2 = data1.drop(['tally_days', 'deposit_type', 'assigned_room_type','is_canceled','adults', 'children', 'babies', \n",
    "                   'avg_rooms_per_night',\n",
    "                   'meal', 'country', 'distribution_channel', 'previous_bookings_not_canceled','deposit_type',\n",
    "                   'days_in_waiting_list', 'reservation_status_date', 'previous_cancellations','expected_arrival_date',\n",
    "                   'lead_time','total_room_nights','adr_mean','adr_median','adr_q01','adr_q1' ,'adr_q2','adr_q3','adr_q4',                       \n",
    "                   'Group','Group3'], axis =1)\n",
    "data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some column names\n",
    "# data1.rename(columns = {'ARRIVAL_DATE_TRANSFORMED':'arrival_date', 'RESERVATION_STATUS_DATE_TRANSFORMED':'reservation_date',\n",
    "#                        'HOTEL':'hotel', 'ARRIVAL_DATE_YEAR':'year','MONTH':'month', \n",
    "#                       'ARRIVAL_DATE_WEEK_NUMBER': 'week', 'ARRIVAL_DATE_DAY_OF_MONTH': 'dom', \n",
    "#                       'RESERVATION_STATUS': 'reservation_status', 'TOTAL_STAY_NIGHTS': 'total_rns',\n",
    "#                       'STAYS_IN_WEEKEND_NIGHTS':'weekend_nights', \n",
    "#                       'STAYS_IN_WEEK_NIGHTS': 'week_nights','TOTAL_GUESTS': 'total_guests','MARKET_SEGMENT':'market_segment', \n",
    "#                       'RESERVED_ROOM_TYPE': 'room_type', 'CUSTOMER_TYPE':'cust_type', 'ADR':'adr'}, inplace = True) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9704628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "data2.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a16e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating separate tables for Optimizing models\n",
    "ca = data2[(data2['hotel'] == 'City Hotel') & (data2['room_type'] == 'A')]\n",
    "cd = data2[(data2['hotel'] == 'City Hotel') & (data2['room_type'] == 'D')]\n",
    "ce = data2[(data2['hotel'] == 'City Hotel') & (data2['room_type'] == 'E')]\n",
    "ra = data2[(data2['hotel'] == 'Resort Hotel') & (data2['room_type'] == 'A')]\n",
    "rd = data2[(data2['hotel'] == 'Resort Hotel') & (data2['room_type'] == 'D')]\n",
    "re = data2[(data2['hotel'] == 'Resort Hotel') & (data2['room_type'] == 'E')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "ca.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e89b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "cd.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adf01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "ce.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "ra.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "rd.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d41d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "re.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9636d80",
   "metadata": {},
   "source": [
    "# Data transformation to disaggregate dates\n",
    "# For Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_transform = data2[['hotel','room_type','arrival_date','total_rns','adr']]\n",
    "data_to_transform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the data by stay date\n",
    "expanded_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the expected arrival date\n",
    "##Need to convert with hotel, room type columns\n",
    "for _, row in data_to_transform.iterrows():\n",
    "    # Get the number of stay dates for the current booking\n",
    "    num_stay_dates = row['total_rns']\n",
    "    \n",
    "    #Create a row for each stay date\n",
    "    expanded_booking = pd.DataFrame({\\\n",
    "        'hotel': row['hotel'],\n",
    "        'room_type' : row ['room_type'],                             \n",
    "        'arrival_date': pd.date_range(start=row['arrival_date'], periods=num_stay_dates),\n",
    "        'total_rns': 1,\n",
    "        'adr': row['adr']\n",
    "    })                                \n",
    "    # Append the stay date information to the new dataframe\n",
    "    expanded_df = pd.concat([expanded_df, expanded_booking], ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the final dataframe by date\n",
    "expanded_df = expanded_df.sort_values('adr')\n",
    "expanded_df = expanded_df.reset_index(drop=True)\n",
    "expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be662c1a",
   "metadata": {},
   "source": [
    "## Building seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "holiday_dates = holidays.CountryHoliday('PT', years=[2020,2021,2022,2023])\n",
    "holidays = {\n",
    "    expected_arrival_date: name\n",
    "    for expected_arrival_date, name in holiday_dates.items()\n",
    "    if name in ['Ano Novo', 'Páscoa', 'Dia de Natal']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename holiday columns\n",
    "expanded_df = expanded_df.rename({'Ano Novo':'new_year','Páscoa':'easter','Dia de Natal':'christmas'},axis=1)\n",
    "\n",
    "holidays = {datetime.date(2023, 1, 1): 'new_year',\n",
    " datetime.date(2023, 3, 27): 'easter',\n",
    " datetime.date(2023, 12, 25): 'christmas',\n",
    " datetime.date(2022, 1, 1): 'new_year',\n",
    " datetime.date(2022, 4, 16): 'easter',\n",
    " datetime.date(2022, 12, 25): 'christmas',\n",
    " datetime.date(2021, 1, 1): 'new_year',\n",
    " datetime.date(2021, 4, 5): 'easter',\n",
    " datetime.date(2021, 12, 25): 'christmas',\n",
    " datetime.date(2020, 1, 1): 'new_year',\n",
    " datetime.date(2020, 4, 5): 'easter',\n",
    " datetime.date(2020, 12, 25): 'christmas'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3247196",
   "metadata": {},
   "source": [
    "# updated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holiday_dates(start_year, end_year):\n",
    "    holidays = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        holidays[datetime.date(year, 1, 1)] = 'new_year'\n",
    "        easter_date = pd.to_datetime(calendar().easter(year)).date()\n",
    "        holidays[easter_date] = 'easter'\n",
    "        holidays[datetime.date(year, 12, 25)] = 'christmas'\n",
    "    return holidays\n",
    "\n",
    "generate_holiday_dates(2020, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faaf989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pre and post ranges for each holiday\n",
    "pre_range_offset = {'new_year': relativedelta(days=-1),\n",
    "                    'easter': relativedelta(days=-2),\n",
    "                    'christmas': relativedelta(days=-3)}\n",
    "\n",
    "post_range_offset = {'new_year': relativedelta(days=1),\n",
    "                     'easter': relativedelta(days=2),\n",
    "                     'christmas': relativedelta(days=3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddae79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b465f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for each holiday\n",
    "for holiday in holidays.values():\n",
    "    expanded_df[holiday] = 0\n",
    " \n",
    " # Set the holiday columns to 1 for matching dates\n",
    "for arrival_date, name in holidays.items():\n",
    "    expanded_df.loc[expanded_df['arrival_date'] == arrival_date, name] = 1\n",
    "\n",
    "    # Set the holiday columns to 1 for pre and post dates\n",
    "    pre_offset = pre_range_offset.get(name)\n",
    "    if pre_offset:\n",
    "        pre_date = pd.to_datetime(arrival_date) + pre_offset\n",
    "        expanded_df.loc[expanded_df['arrival_date'] == pre_date.strftime('%Y-%m-%d'), name] = 1\n",
    "\n",
    "    post_offset = post_range_offset.get(name)\n",
    "    if post_offset:\n",
    "        post_date = pd.to_datetime(arrival_date) + post_offset\n",
    "        expanded_df.loc[expanded_df['arrival_date'] == post_date.strftime('%Y-%m-%d'), name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1028e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f23fa498",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd218944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dow, month to data\n",
    "expanded_df['dow'] = expanded_df.arrival_date.dt.strftime('%A')\n",
    "expanded_df['month'] = expanded_df.arrival_date.dt.strftime('%B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116102e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PRICING MODEL 1 (WITHOUT HOLIDAYS)\n",
    "#Remove holidays for our main model\n",
    "non_holidays = expanded_df[expanded_df[['new_year', 'easter', 'christmas']].sum(axis=1) == 0]\n",
    "non_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece32944",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Our optimization will be done at the DOW and Month Level. As a reminder, we are trying to model the relationship between Price and Demand so that we can answer the below question: if I charge X , how many rooms will I sell? To begin with, we need to get the demand levels for each DOW and month as well as create a table counting how many times each price point appears for each DOW and month.\n",
    "\n",
    "##ADD HOTEL, ROOM TYPE TO GROUPINGS\n",
    "daily_rns = non_holidays.groupby(['arrival_date','dow','month']).agg({'total_rns':'sum'}).reset_index() # get total stays per day\n",
    "daily_rns = daily_rns.groupby(['dow','month']).agg({'total_rns':['sum','mean','median']}).reset_index() # get Rns metrics by Dow & Month\n",
    "daily_rns.columns = ['_'.join(col) for col in daily_rns.columns] #remove multi level column\n",
    "daily_rns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45117dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Counting rate frequency\n",
    "##In the next step, we will count how often each adr (price the guest paid) appears for each DOW and month.\n",
    "# create rate frequency table\n",
    "adr_frequency = non_holidays.groupby(['dow','month','adr']).agg({'total_rns':'sum'})\n",
    "adr_frequency.reset_index(inplace=True)\n",
    "adr_frequency.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad8896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86f94deb",
   "metadata": {},
   "source": [
    "# Data transformation to dis-aggregate dates- \n",
    "# For City Hotel room type A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_data_to_transform = ca[['hotel','room_type','arrival_date','total_rns','adr']]\n",
    "ca_data_to_transform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the data by stay date\n",
    "ca_expanded_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the expected arrival date\n",
    "##Need to convert with hotel, room type columns\n",
    "for _, row in ca_data_to_transform.iterrows():\n",
    "    # Get the number of stay dates for the current booking\n",
    "    num_stay_dates = row['total_rns']\n",
    "    \n",
    "    #Create a row for each stay date\n",
    "    ca_expanded_booking = pd.DataFrame({\n",
    "        'hotel': row['hotel'],\n",
    "        'room_type' : row ['room_type'], \n",
    "        'arrival_date': pd.date_range(start=row['arrival_date'], periods=num_stay_dates),\n",
    "        'total_rns': 1,\n",
    "        'adr': row['adr'] \n",
    "    })\n",
    "    # Append the stay date information to the new dataframe\n",
    "    ca_expanded_df = pd.concat([ca_expanded_df, ca_expanded_booking], ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ba954",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_expanded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_expanded_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the final dataframe by date\n",
    "ca_expanded_df = ca_expanded_df.sort_values('arrival_date')\n",
    "ca_expanded_df = ca_expanded_df.reset_index(drop=True)\n",
    "ca_expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_expanded_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
