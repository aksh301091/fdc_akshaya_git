{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0e4ded",
   "metadata": {},
   "source": [
    "# Price Optimization for Hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fb829",
   "metadata": {},
   "source": [
    "# Getting snowflake related packages installed and imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b222af8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting snowflake-snowpark-python==1.9.0\n",
      "  Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fosforio\n",
      "  Downloading fosforio-1.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fosforml\n",
      "  Downloading fosforml-1.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting python-dateutil\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting holidays\n",
      "  Downloading holidays-0.51-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading snowflake_connector_python-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setuptools>=40.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading setuptools-70.1.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting wheel (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.1.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyyaml (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cffi<2.0.0,>=1.9 (from snowflake-connector-python[pandas])\n",
      "  Downloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting cryptography<43.0.0,>=3.1.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading cryptography-42.0.8-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting pyOpenSSL<25.0.0,>=16.2.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading pyOpenSSL-24.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyjwt<3.0.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pytz (from snowflake-connector-python[pandas])\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting requests<3.0.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting packaging (from snowflake-connector-python[pandas])\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from snowflake-connector-python[pandas])\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from snowflake-connector-python[pandas])\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting platformdirs<5.0.0,>=2.6.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tomlkit (from snowflake-connector-python[pandas])\n",
      "  Downloading tomlkit-0.12.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting urllib3<2.0.0,>=1.21.1 (from snowflake-connector-python[pandas])\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m238.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow (from snowflake-connector-python[pandas])\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "INFO: pip is looking at multiple versions of fosforml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fosforml\n",
      "  Downloading fosforml-1.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting cloudpickle<=2.0.0,>=1.6.0 (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting requests-toolbelt==0.9.1 (from fosforml)\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting shutils==0.1.0 (from fosforml)\n",
      "  Downloading shutils-0.1.0.tar.gz (2.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml (from snowflake-snowpark-python==1.9.0)\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting mosaic-utils (from fosforml)\n",
      "  Downloading mosaic_utils-1.0.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<2.0.0,>=1.21.1 (from snowflake-connector-python[pandas])\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m222.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting configparser (from shutils==0.1.0->fosforml)\n",
      "  Downloading configparser-7.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pymysql (from shutils==0.1.0->fosforml)\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading pillow-10.3.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m248.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting six>=1.5 (from python-dateutil)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pycparser (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas])\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib)\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading snowflake_snowpark_python-1.9.0-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fosforio-1.0.2-py3-none-any.whl (20 kB)\n",
      "Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m244.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fosforml-1.0.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m214.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m334.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m239.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m317.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m240.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m156.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m342.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m338.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m268.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading holidays-0.51-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m288.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m286.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m309.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.7/444.7 kB\u001b[0m \u001b[31m321.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m302.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m340.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-42.0.8-cp37-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m153.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading fonttools-4.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m238.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m331.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m275.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m248.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m160.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m257.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m277.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m324.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m268.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m150.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-70.1.1-py3-none-any.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.3/883.3 kB\u001b[0m \u001b[31m354.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading snowflake_connector_python-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m161.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m335.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mosaic_utils-1.0.2-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m278.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m156.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\n",
      "Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m253.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Downloading configparser-7.0.0-py3-none-any.whl (16 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m292.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m245.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: shutils\n",
      "  Building wheel for shutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for shutils: filename=shutils-0.1.0-py3-none-any.whl size=3274 sha256=e7f01416363294d4b65de5819c929bca878cbceba093d9be2dc5fda571b47b8a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_h4c5gu3/wheels/c7/06/1a/4df9f03d511efd19a10f2086f67c38cf14757b555f4f24bd6e\n",
      "Successfully built shutils\n",
      "Installing collected packages: sortedcontainers, pytz, asn1crypto, zipp, wheel, urllib3, tzdata, typing-extensions, tqdm, tomlkit, threadpoolctl, six, setuptools, pyyaml, pyparsing, pymysql, pyjwt, pycparser, platformdirs, pillow, packaging, nvidia-nccl-cu12, numpy, kiwisolver, joblib, idna, fonttools, filelock, cycler, configparser, cloudpickle, charset-normalizer, certifi, shutils, scipy, requests, python-dateutil, pyarrow, importlib-resources, contourpy, cffi, xgboost, scikit-learn, requests-toolbelt, pandas, matplotlib, holidays, cryptography, seaborn, pyOpenSSL, mosaic-utils, fosforio, snowflake-connector-python, fosforml, snowflake-snowpark-python\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab 3.2.4 requires jupyter-server~=1.4, but you have jupyter-server 2.0.0a1 which is incompatible.\n",
      "jupyterlab-server 2.25.4 requires jsonschema>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "mosaic-ai-client 1.0.0 requires matplotlib==3.1.1, but you have matplotlib 3.7.5 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires Flask==2.1.1; python_version >= \"3.7\", but you have flask 2.2.5 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires matplotlib==3.6.0; python_version >= \"3.8\", but you have matplotlib 3.7.5 which is incompatible.\n",
      "openapi-schema-validator 0.6.2 requires jsonschema<5.0.0,>=4.19.1, but you have jsonschema 3.2.0 which is incompatible.\n",
      "openapi-spec-validator 0.7.1 requires jsonschema<5.0.0,>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires cryptography<39.0.0,>=3.1.0, but you have cryptography 42.0.8 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires packaging<24,>=20.9, but you have packaging 24.1 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires pandas<2,>=1.0.0, but you have pandas 2.0.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires xgboost<2,>=1.7.3, but you have xgboost 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asn1crypto-1.5.1 certifi-2024.6.2 cffi-1.16.0 charset-normalizer-3.3.2 cloudpickle-1.6.0 configparser-7.0.0 contourpy-1.1.1 cryptography-42.0.8 cycler-0.12.1 filelock-3.15.4 fonttools-4.53.0 fosforio-1.0.2 fosforml-1.0.0 holidays-0.51 idna-3.7 importlib-resources-6.4.0 joblib-1.4.2 kiwisolver-1.4.5 matplotlib-3.7.5 mosaic-utils-1.0.2 numpy-1.24.4 nvidia-nccl-cu12-2.22.3 packaging-24.1 pandas-2.0.0 pillow-10.3.0 platformdirs-4.2.2 pyOpenSSL-24.1.0 pyarrow-16.1.0 pycparser-2.22 pyjwt-2.8.0 pymysql-1.1.1 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytz-2024.1 pyyaml-6.0 requests-2.32.3 requests-toolbelt-0.9.1 scikit-learn-1.2.1 scipy-1.10.1 seaborn-0.13.2 setuptools-70.1.1 shutils-0.1.0 six-1.16.0 snowflake-connector-python-3.11.0 snowflake-snowpark-python-1.9.0 sortedcontainers-2.4.0 threadpoolctl-3.5.0 tomlkit-0.12.5 tqdm-4.66.4 typing-extensions-4.12.2 tzdata-2024.1 urllib3-1.26.15 wheel-0.43.0 xgboost-2.1.0 zipp-3.19.2\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/sortedcontainers already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/sortedcontainers-2.4.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pytz-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/asn1crypto already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/asn1crypto-1.5.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/zipp already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/zipp-3.19.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/wheel already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/wheel-0.43.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3-1.26.15.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tzdata already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tzdata-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/typing_extensions-4.12.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tqdm-4.66.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tomlkit already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tomlkit-0.12.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl-3.5.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/six-1.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/distutils-precedence.pth already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/_distutils_hack already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pkg_resources already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/setuptools already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/setuptools-70.1.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/PyYAML-6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pyparsing already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pyparsing-3.1.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pymysql already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/PyMySQL-1.1.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/jwt already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/PyJWT-2.8.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pycparser already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pycparser-2.22.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/platformdirs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/platformdirs-4.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pillow.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/PIL already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pillow-10.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/packaging already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/packaging-24.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/nvidia already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/nvidia_nccl_cu12-2.22.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-1.24.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/kiwisolver-1.4.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/kiwisolver already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib-1.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/fontTools already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/fonttools-4.53.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/filelock already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/filelock-3.15.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cycler already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cycler-0.12.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/backports already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/configparser-7.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle-1.6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi-2024.6.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils-0.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.10.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/requests-2.32.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pyarrow already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pyarrow-16.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/importlib_resources already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/importlib_resources-6.4.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/contourpy-1.1.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/contourpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/_cffi_backend.cpython-38-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cffi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cffi-1.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/xgboost already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/xgboost-2.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/xgboost.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn-1.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt-0.9.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pandas-2.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pandas already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/matplotlib-3.7.5-py3.8-nspkg.pth already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pylab.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/matplotlib-3.7.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/mpl_toolkits already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/matplotlib already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/holidays already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/holidays-0.51.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cryptography already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/cryptography-42.0.8.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/seaborn already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/seaborn-0.13.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/OpenSSL already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/pyOpenSSL-24.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/fosforio already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/fosforio-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/snowflake already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/snowflake_connector_python-3.11.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/fosforml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/tests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/fosforml-1.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/snowflake_snowpark_python-1.9.0-py3.11-nspkg.pth already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/snowflake_snowpark_python-1.9.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /tmp/pip_packages/share already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fosforml 1.0.1 requires cloudpickle==3.0.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
      "fosforml 1.0.1 requires urllib3==2.2.1, but you have urllib3 1.26.19 which is incompatible.\n",
      "jupyterlab 3.2.4 requires jupyter-server~=1.4, but you have jupyter-server 2.0.0a1 which is incompatible.\n",
      "jupyterlab-server 2.25.4 requires jsonschema>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "mosaic-ai-client 1.0.0 requires matplotlib==3.1.1, but you have matplotlib 3.7.5 which is incompatible.\n",
      "mosaic-ai-client 1.0.0 requires requests-toolbelt==0.9.1, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires Flask==2.1.1; python_version >= \"3.7\", but you have flask 2.2.5 which is incompatible.\n",
      "mosaic-ai-serving 1.0.0 requires matplotlib==3.6.0; python_version >= \"3.8\", but you have matplotlib 3.7.5 which is incompatible.\n",
      "openapi-schema-validator 0.6.2 requires jsonschema<5.0.0,>=4.19.1, but you have jsonschema 3.2.0 which is incompatible.\n",
      "openapi-spec-validator 0.7.1 requires jsonschema<5.0.0,>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "shap 0.44.1 requires tqdm>=4.27.0, but you have tqdm 4.8.3 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires cryptography<39.0.0,>=3.1.0, but you have cryptography 42.0.8 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires packaging<24,>=20.9, but you have packaging 24.1 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires pandas<2,>=1.0.0, but you have pandas 2.0.0 which is incompatible.\n",
      "snowflake-ml-python 1.0.1 requires xgboost<2,>=1.7.3, but you have xgboost 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"snowflake-connector-python[pandas]\" \"snowflake-snowpark-python[pandas]\" snowflake-snowpark-python==1.9.0 fosforio fosforml numpy pandas matplotlib scikit-learn xgboost seaborn python-dateutil tqdm holidays\n",
    "!pip install --upgrade --q snowflake-snowpark-python==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab3f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c504d4e",
   "metadata": {},
   "source": [
    "# Restart kernal and continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14007d14",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba1948d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cryptography.__about__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/301f4977-676a-4c39-9ca7-6a54313650a1/3.8/snowflake/snowpark/__init__.py:46\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VERSION\n\u001b[1;32m     43\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m VERSION \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_job\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncJob\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolumn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CaseExpr, Column\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/301f4977-676a-4c39-9ca7-6a54313650a1/3.8/snowflake/snowpark/async_job.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator, List, Literal, Optional, Union\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pandas\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_scan_statement\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowflake_plan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Query\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/__init__.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NullHandler\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SnowflakeConnection\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcursor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictCursor\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdbapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     BINARY,\n\u001b[1;32m     23\u001b[0m     DATETIME,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     TimestampFromTicks,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/connection.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Generator, Iterable, Iterator, NamedTuple, Sequence\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UUID\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcryptography\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhazmat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_backend\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcryptography\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhazmat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcryptography\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhazmat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimitives\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masymmetric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrsa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RSAPrivateKey\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/301f4977-676a-4c39-9ca7-6a54313650a1/3.8/cryptography/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This file is dual licensed under the terms of the Apache License, Version\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 2.0, and the BSD License. See the LICENSE file in the root of this repository\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# for complete details.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcryptography\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__about__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __author__, __copyright__, __version__\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__author__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__copyright__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cryptography.__about__'"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforio import snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81490294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "import configparser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03df6c",
   "metadata": {},
   "source": [
    "# Getting data from snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307b6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get snowflake connection object with a default snowflake connection created by the user, if available.\n",
    "snowflake.get_connection(\"TTH_REV_OPT_CXN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ef799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = snowflake.get_dataframe(\"BOOKINGS_TRANSFORMED\")\n",
    "df['ADR']= round(df['ADR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download star schema tables\n",
    "meal = \n",
    "customer = \n",
    "channel = \n",
    "room = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [x.lower() for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98990657",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x=\"reserved_room_type\", y=\"adr\", hue= 'is_canceled', color=\".8\", linecolor=\"#137\", linewidth=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles_data = df.groupby(['hotel', 'reserved_room_type'])['adr'].agg(\n",
    "    adr_mean='mean',\n",
    "    adr_median='median',\n",
    "    adr_q01=lambda x: x.quantile(0.10),\n",
    "    adr_q1=lambda x: x.quantile(0.25),\n",
    "    adr_q2=lambda x: x.quantile(0.50),\n",
    "    adr_q3=lambda x: x.quantile(0.75),\n",
    "    adr_q4=lambda x: x.quantile(1.00)\n",
    ").reset_index()\n",
    "print(quartiles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the quartiles data with the original data\n",
    "merged_data = pd.merge(df, quartiles_data, on=['hotel', 'reserved_room_type'])\n",
    "\n",
    "print(\"\\nMerged Data:\")\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = merged_data[(merged_data['adr'] > merged_data['adr_q01']) & \n",
    "                            (merged_data['adr'] < merged_data['adr_q4'])]\n",
    "\n",
    "print(\"\\nFiltered Data:\")\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be354cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some column names\n",
    "filtered_data.rename(columns = {'arrival_date_transformed':'arrival_date', \n",
    "                                'reservation_status_date_transformed':'reservation_date',\n",
    "                                'arrival_date_year':'year','arrival_date_week_number': 'week', 'arrival_date_day_of_month': 'dom', \n",
    "                                'total_stay_nights': 'total_rns',\n",
    "                                'stays_in_weekend_nights':'weekend_nights', 'stays_in_week_nights': 'week_nights',\n",
    "                                'reserved_room_type': 'room_type', 'customer_type':'cust_type', 'ADR':'adr'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=filtered_data, x=\"room_type\", y=\"adr\", hue= 'is_canceled', color=\".8\", linecolor=\"#137\", linewidth=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=filtered_data, x=\"room_type\", y=\"adr\", hue= 'market_segment', \n",
    "            fill=False, gap=.1, linecolor=\"#137\", linewidth=.75, color= 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined column for the plot\n",
    "filtered_data['Group'] = filtered_data['hotel'] + ' - ' + filtered_data['room_type']\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Create boxplot\n",
    "sns.boxplot(x='Group', y='adr', data=filtered_data)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Boxplot of ADR by Hotel and Reserved Room Type')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('adr')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e978f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined column for the plot\n",
    "filtered_data['Group3'] = filtered_data['room_type'] + ' - ' + filtered_data['reservation_status']\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Create boxplot\n",
    "sns.boxplot(x='Group3', y='adr', data=filtered_data)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Boxplot of ADR by CANCELED and Reserved Room Type')\n",
    "plt.xlabel('Group3')\n",
    "plt.ylabel('adr')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218ea6",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf15b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing cancelled transactions to model based on transactions that customers preferred to actually complete\n",
    "df0 = filtered_data[(filtered_data['is_canceled'] == 0) & (filtered_data['reservation_status'] !='No-Show')] \n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99536bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding what market segments need to be considered\n",
    "df0.groupby(['hotel','room_type','market_segment']).agg({'adr':'mean','reservation_date':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8712a",
   "metadata": {},
   "source": [
    "#We will filter for room _types A,D,E and will filter OUT \"Complementary\" market_segment. retain both City and resort hotel types.\n",
    "##\"For our pricing model, we need to count the frequency of each price point\n",
    "\n",
    "##Moreover, Hotels have different room types with different prices. To create our pricing model, we need to optimize the room type level. We are interested in the room type reserved as this captures the price the guest was willing to pay for that room type. It is common for hotels to overbook the lower room types and give free upgrades to higher room types. Therefore, any pricing analysis must be done at the reserved room type and not the stayed room type.\n",
    "\n",
    "##Unfortunately, there is not much description of the reserved room type, but by looking at the price points, we can infer that A &B are the lowest price points. The rest of the room types command a higher ADR, which means they likely are upgraded rooms. In another workbook, we will explore how to price those premium rooms, but for the moment, let's focus on room types A & D, E.\n",
    "\n",
    "#filter data to only Direct and Online TA and room typ A&B\n",
    "##Create individual datasets for City, Resort, room types: A,D,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = df0[(df0['market_segment'] != 'Complementary') & (df0['market_segment'] != 'Corporate') \n",
    "#           & (df0['market_segment'] !='Aviation')]\n",
    "data1 = df0[~df0['market_segment'].isin(['Complementary', 'Corporate', 'Aviation'])]\n",
    "#data1 = df0[(df0['room_type'] == 'A') |(df0['room_type'] == 'D') | (df0['room_type'] == 'E')]\n",
    "data1.room_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf42531",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.market_segment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031841de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding what market segments need to be considered\n",
    "data1.groupby(['hotel','room_type','market_segment']).agg({'adr':'mean','reservation_date':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b66650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "data1.groupby(['arrival_date']).agg({'adr':'mean','reservation_date':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf489a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unrequired columns\n",
    "data2 = data1.drop(['tally_days', 'deposit_type', 'assigned_room_type','is_canceled','adults', 'children', 'babies', \n",
    "                   'avg_rooms_per_night',\n",
    "                   'meal', 'country', 'distribution_channel', 'previous_bookings_not_canceled','deposit_type',\n",
    "                   'days_in_waiting_list', 'reservation_status_date', 'previous_cancellations','expected_arrival_date',\n",
    "                   'lead_time','total_room_nights','adr_mean','adr_median','adr_q01','adr_q1' ,'adr_q2','adr_q3','adr_q4',                       \n",
    "                   'Group','Group3'], axis =1)\n",
    "data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some column names\n",
    "# data1.rename(columns = {'ARRIVAL_DATE_TRANSFORMED':'arrival_date', 'RESERVATION_STATUS_DATE_TRANSFORMED':'reservation_date',\n",
    "#                        'HOTEL':'hotel', 'ARRIVAL_DATE_YEAR':'year','MONTH':'month', \n",
    "#                       'ARRIVAL_DATE_WEEK_NUMBER': 'week', 'ARRIVAL_DATE_DAY_OF_MONTH': 'dom', \n",
    "#                       'RESERVATION_STATUS': 'reservation_status', 'TOTAL_STAY_NIGHTS': 'total_rns',\n",
    "#                       'STAYS_IN_WEEKEND_NIGHTS':'weekend_nights', \n",
    "#                       'STAYS_IN_WEEK_NIGHTS': 'week_nights','TOTAL_GUESTS': 'total_guests','MARKET_SEGMENT':'market_segment', \n",
    "#                       'RESERVED_ROOM_TYPE': 'room_type', 'CUSTOMER_TYPE':'cust_type', 'ADR':'adr'}, inplace = True) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9704628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "data2.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a16e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating separate tables for Optimizing models\n",
    "ca = data2[(data2['hotel'] == 'City Hotel') & (data2['room_type'] == 'A')]\n",
    "cd = data2[(data2['hotel'] == 'City Hotel') & (data2['room_type'] == 'D')]\n",
    "ce = data2[(data2['hotel'] == 'City Hotel') & (data2['room_type'] == 'E')]\n",
    "ra = data2[(data2['hotel'] == 'Resort Hotel') & (data2['room_type'] == 'A')]\n",
    "rd = data2[(data2['hotel'] == 'Resort Hotel') & (data2['room_type'] == 'D')]\n",
    "re = data2[(data2['hotel'] == 'Resort Hotel') & (data2['room_type'] == 'E')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "ca.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e89b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "cd.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adf01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "ce.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "ra.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "rd.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d41d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking num dates\n",
    "re.groupby(['arrival_date']).agg({'adr':'mean','hotel':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9636d80",
   "metadata": {},
   "source": [
    "# Data transformation to disaggregate dates\n",
    "# For Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_transform = data2[['hotel','room_type','arrival_date','total_rns','adr']]\n",
    "data_to_transform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the data by stay date\n",
    "expanded_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the expected arrival date\n",
    "##Need to convert with hotel, room type columns\n",
    "for _, row in data_to_transform.iterrows():\n",
    "    # Get the number of stay dates for the current booking\n",
    "    num_stay_dates = row['total_rns']\n",
    "    \n",
    "    #Create a row for each stay date\n",
    "    expanded_booking = pd.DataFrame({\\\n",
    "        'hotel': row['hotel'],\n",
    "        'room_type' : row ['room_type'],                             \n",
    "        'arrival_date': pd.date_range(start=row['arrival_date'], periods=num_stay_dates),\n",
    "        'total_rns': 1,\n",
    "        'adr': row['adr']\n",
    "    })                                \n",
    "    # Append the stay date information to the new dataframe\n",
    "    expanded_df = pd.concat([expanded_df, expanded_booking], ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the final dataframe by date\n",
    "expanded_df = expanded_df.sort_values('adr')\n",
    "expanded_df = expanded_df.reset_index(drop=True)\n",
    "expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be662c1a",
   "metadata": {},
   "source": [
    "## Building seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "holiday_dates = holidays.CountryHoliday('PT', years=[2020,2021,2022,2023])\n",
    "holidays = {\n",
    "    expected_arrival_date: name\n",
    "    for expected_arrival_date, name in holiday_dates.items()\n",
    "    if name in ['Ano Novo', 'Páscoa', 'Dia de Natal']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename holiday columns\n",
    "expanded_df = expanded_df.rename({'Ano Novo':'new_year','Páscoa':'easter','Dia de Natal':'christmas'},axis=1)\n",
    "\n",
    "holidays = {datetime.date(2023, 1, 1): 'new_year',\n",
    " datetime.date(2023, 3, 27): 'easter',\n",
    " datetime.date(2023, 12, 25): 'christmas',\n",
    " datetime.date(2022, 1, 1): 'new_year',\n",
    " datetime.date(2022, 4, 16): 'easter',\n",
    " datetime.date(2022, 12, 25): 'christmas',\n",
    " datetime.date(2021, 1, 1): 'new_year',\n",
    " datetime.date(2021, 4, 5): 'easter',\n",
    " datetime.date(2021, 12, 25): 'christmas',\n",
    " datetime.date(2020, 1, 1): 'new_year',\n",
    " datetime.date(2020, 4, 5): 'easter',\n",
    " datetime.date(2020, 12, 25): 'christmas'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3247196",
   "metadata": {},
   "source": [
    "# updated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holiday_dates(start_year, end_year):\n",
    "    holidays = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        holidays[datetime.date(year, 1, 1)] = 'new_year'\n",
    "        easter_date = pd.to_datetime(calendar().easter(year)).date()\n",
    "        holidays[easter_date] = 'easter'\n",
    "        holidays[datetime.date(year, 12, 25)] = 'christmas'\n",
    "    return holidays\n",
    "\n",
    "generate_holiday_dates(2020, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faaf989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pre and post ranges for each holiday\n",
    "pre_range_offset = {'new_year': relativedelta(days=-1),\n",
    "                    'easter': relativedelta(days=-2),\n",
    "                    'christmas': relativedelta(days=-3)}\n",
    "\n",
    "post_range_offset = {'new_year': relativedelta(days=1),\n",
    "                     'easter': relativedelta(days=2),\n",
    "                     'christmas': relativedelta(days=3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddae79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b465f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for each holiday\n",
    "for holiday in holidays.values():\n",
    "    expanded_df[holiday] = 0\n",
    " \n",
    " # Set the holiday columns to 1 for matching dates\n",
    "for arrival_date, name in holidays.items():\n",
    "    expanded_df.loc[expanded_df['arrival_date'] == arrival_date, name] = 1\n",
    "\n",
    "    # Set the holiday columns to 1 for pre and post dates\n",
    "    pre_offset = pre_range_offset.get(name)\n",
    "    if pre_offset:\n",
    "        pre_date = pd.to_datetime(arrival_date) + pre_offset\n",
    "        expanded_df.loc[expanded_df['arrival_date'] == pre_date.strftime('%Y-%m-%d'), name] = 1\n",
    "\n",
    "    post_offset = post_range_offset.get(name)\n",
    "    if post_offset:\n",
    "        post_date = pd.to_datetime(arrival_date) + post_offset\n",
    "        expanded_df.loc[expanded_df['arrival_date'] == post_date.strftime('%Y-%m-%d'), name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1028e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f23fa498",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd218944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dow, month to data\n",
    "expanded_df['dow'] = expanded_df.arrival_date.dt.strftime('%A')\n",
    "expanded_df['month'] = expanded_df.arrival_date.dt.strftime('%B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116102e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PRICING MODEL 1 (WITHOUT HOLIDAYS)\n",
    "#Remove holidays for our main model\n",
    "non_holidays = expanded_df[expanded_df[['new_year', 'easter', 'christmas']].sum(axis=1) == 0]\n",
    "non_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece32944",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Our optimization will be done at the DOW and Month Level. As a reminder, we are trying to model the relationship between Price and Demand so that we can answer the below question: if I charge X , how many rooms will I sell? To begin with, we need to get the demand levels for each DOW and month as well as create a table counting how many times each price point appears for each DOW and month.\n",
    "\n",
    "##ADD HOTEL, ROOM TYPE TO GROUPINGS\n",
    "daily_rns = non_holidays.groupby(['arrival_date','dow','month']).agg({'total_rns':'sum'}).reset_index() # get total stays per day\n",
    "daily_rns = daily_rns.groupby(['dow','month']).agg({'total_rns':['sum','mean','median']}).reset_index() # get Rns metrics by Dow & Month\n",
    "daily_rns.columns = ['_'.join(col) for col in daily_rns.columns] #remove multi level column\n",
    "daily_rns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45117dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Counting rate frequency\n",
    "##In the next step, we will count how often each adr (price the guest paid) appears for each DOW and month.\n",
    "# create rate frequency table\n",
    "adr_frequency = non_holidays.groupby(['dow','month','adr']).agg({'total_rns':'sum'})\n",
    "adr_frequency.reset_index(inplace=True)\n",
    "adr_frequency.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad8896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86f94deb",
   "metadata": {},
   "source": [
    "# Data transformation to dis-aggregate dates- \n",
    "# For City Hotel room type A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_data_to_transform = ca[['hotel','room_type','arrival_date','total_rns','adr']]\n",
    "ca_data_to_transform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the data by stay date\n",
    "ca_expanded_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the expected arrival date\n",
    "##Need to convert with hotel, room type columns\n",
    "for _, row in ca_data_to_transform.iterrows():\n",
    "    # Get the number of stay dates for the current booking\n",
    "    num_stay_dates = row['total_rns']\n",
    "    \n",
    "    #Create a row for each stay date\n",
    "    ca_expanded_booking = pd.DataFrame({\n",
    "        'hotel': row['hotel'],\n",
    "        'room_type' : row ['room_type'], \n",
    "        'arrival_date': pd.date_range(start=row['arrival_date'], periods=num_stay_dates),\n",
    "        'total_rns': 1,\n",
    "        'adr': row['adr'] \n",
    "    })\n",
    "    # Append the stay date information to the new dataframe\n",
    "    ca_expanded_df = pd.concat([ca_expanded_df, ca_expanded_booking], ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ba954",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_expanded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_expanded_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the final dataframe by date\n",
    "ca_expanded_df = ca_expanded_df.sort_values('arrival_date')\n",
    "ca_expanded_df = ca_expanded_df.reset_index(drop=True)\n",
    "ca_expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_expanded_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
