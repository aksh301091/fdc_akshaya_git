{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9559c1a",
   "metadata": {},
   "source": [
    "## Olympics Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92494cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"snowflake-connector-python[pandas]\" \"snowflake-snowpark-python[pandas]\" snowflake-snowpark-python==1.9.0 fosforio fosforml numpy pandas matplotlib scikit-learn xgboost seaborn python-dateutil tqdm holidays faker\n",
    "!pip install --upgrade --q snowflake-snowpark-python==1.9.0\n",
    "!pip uninstall urllib3 -y\n",
    "!pip install urllib3==1.26.15\n",
    "!pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa438d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ac74e",
   "metadata": {},
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fed65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforio import snowflake\n",
    "from fosforml import *\n",
    "from fosforml.constants import MLModelFlavours\n",
    "from fosforio import get_dataframe\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from joblib import dump, load\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "from time import sleep\n",
    "import configparser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from dateutil.easter import easter\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47834e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "\n",
    "snowflake.get_connection(connection_name=\"ME_OLYMPICS_CNX\")\n",
    "data = get_dataframe(\"ATHLETE_EVENTS\")\n",
    "data\n",
    "data_bkp = get_dataframe(\"ATHLETE_EVENTS\")\n",
    "data_bkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82db46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f0dcf8e",
   "metadata": {},
   "source": [
    "#EDA\n",
    "data['AGE'].value_counts().to_dict()\n",
    "data.info()\n",
    "data['AGE'][data['AGE']!='NA'].astype(int).median()\n",
    "data['WEIGHT'][data['WEIGHT']!='NA'].astype(float).median()\n",
    "data['HEIGHT'][data['HEIGHT']!='NA'].astype(int).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.lower()\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'name', 'games','event', 'noc', 'weight'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b5fda",
   "metadata": {},
   "source": [
    "data['WEIGHT'] = data['WEIGHT'].replace('NA',str(float(data['WEIGHT'][data['WEIGHT']!='NA'].astype(float).median()))).astype(float)\n",
    "\n",
    "data['WEIGHT'].value_counts().to_dict()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ce4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'] = data['age'].replace('NA',str(int(data['age'][data['age']!='NA'].astype(int).median()))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccace873",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['height'] = data['height'].replace('NA',str(int(data['height'][data['height']!='NA'].astype(int).median()))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d0249",
   "metadata": {},
   "source": [
    "# CREATING LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6414119",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['medal'].value_counts()\n",
    "data['medals']= data['medal'].map({'Gold': 3, 'Silver': 2 , 'Bronze': 1, 'NA' : 0}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333207c",
   "metadata": {},
   "source": [
    "data['medals'].value_counts()\n",
    "data['medal'] = data['medal'].apply(lambda x: 1 if str(x) != 'nan' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a copy of data at this stage\n",
    "data_bkp_step2 = data.copy(deep= True)\n",
    "data = data.drop(['medal'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns as necessary,\n",
    "#weight correlated with height, hence dropping\n",
    "#data.info()\n",
    "# data_bkp_step2.info()\n",
    "# data['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17b996",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting datatypes\n",
    "data = data.astype({'age': int, 'height': int})\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ac596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks\n",
    "data.isna().mean()\n",
    "print(\"Total missing values:\", data.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ab0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bkp_step3= data.copy(deep= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_bkp_step3.info()\n",
    "#data['year'].value_counts().sort_values()\n",
    "# data['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['season'] == 'Summer'].reset_index()\n",
    "data = data[data['year'] > 1960].reset_index()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['season'].value_counts()\n",
    "#data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([ 'level_0', 'index'], axis=1)\n",
    "data = data[data['sport'].map(data['sport'].value_counts()) > 3000]\n",
    "data = data[data['team'].map(data['team'].value_counts()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a97d4e",
   "metadata": {},
   "source": [
    "# Checks\n",
    "data['sport'].value_counts()\n",
    "data['medals'].value_counts()\n",
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825283f",
   "metadata": {},
   "source": [
    "# Over all filters:\n",
    "Season: Summer only\n",
    "Year: >1960 only\n",
    "SPorts that had more than 1500 values\n",
    "Team: that had more than 2 values\n",
    "    removed rep. columns like noc, event, etc.\n",
    "    removing weight column as it is highly correlated to height column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy data\n",
    "data_bkp_step4= data.copy(deep= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e8ac4",
   "metadata": {},
   "source": [
    "# Encoding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['medals'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{column: len(data[column].unique()) for column in data.select_dtypes('object').columns}\n",
    "data = data.drop(['season'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(df, columns, positive_values):\n",
    "    df = df.copy()\n",
    "    for column, positive_value in zip(columns, positive_values):\n",
    "        df[column] = df[column].apply(lambda x: 1 if x == positive_value else 0)\n",
    "    return df\n",
    "\n",
    "def onehot_encode(df, columns, prefixes):\n",
    "    df = df.copy()\n",
    "    for column, prefix in zip(columns, prefixes):\n",
    "        dummies = pd.get_dummies(df[column], prefix=prefix)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52576e8b",
   "metadata": {},
   "source": [
    "#df = binary_encode(\n",
    "    f_data2,\n",
    "    columns=['sex', 'season'],\n",
    "    positive_values=['M', 'Summer']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = onehot_encode(\n",
    "    data,\n",
    "    columns=['sex','team', 'city', 'sport'],\n",
    "    prefixes=['sex','t', 'c', 's']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remaining non-numeric columns:\", len(df1.select_dtypes('object').columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['season'].info()\n",
    "#df1 = df1.drop(['season'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593779b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index()\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6762abc",
   "metadata": {},
   "source": [
    "# Visualizing Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df1.loc[:, :'medals'].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, vmin=-1.0, cmap='mako')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a44941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_data = data.groupby('team', 'sport')['medals'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70389aee",
   "metadata": {},
   "source": [
    "# Splitting/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56afdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,  classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "#plot_confusion_matrix not working under skleearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95121b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df1['medals'].copy()\n",
    "X = df1.drop('medals', axis=1).copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ee2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- XGBoost model v1 ----------\n",
    "## base run of model with default hyperparameters\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', \n",
    "                            num_class=3, \n",
    "                            missing=1, \n",
    "                            early_stopping_rounds=10, \n",
    "                            eval_metric=['merror','mlogloss'], \n",
    "                            seed=42)\n",
    "\n",
    "xgb_clf.fit(X_train, \n",
    "            y_train,\n",
    "            verbose=0, # set to 1 to see xgb training round intermediate results\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "# preparing evaluation metric plots\n",
    "results = xgb_clf.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# xgboost 'mlogloss' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('mlogloss')\n",
    "plt.title('GridSearchCV XGBoost mlogloss')\n",
    "plt.show()\n",
    "\n",
    "# xgboost 'merror' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['merror'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['merror'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('merror')\n",
    "plt.title('GridSearchCV XGBoost merror')\n",
    "plt.show()\n",
    "\n",
    "## ---------- Model Classification Report ----------\n",
    "## get predictions and create model quality report\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('\\n-------------------- Key Metrics --------------------')\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('---------------------- XGBoost ----------------------') # unnecessary fancy styling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089635a7",
   "metadata": {},
   "source": [
    "# XGBoost Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- XGBoost model v2 ----------\n",
    "## second run of model with \"optimized\" hyperparameters \n",
    "\n",
    "# declaring and fitting xgb classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', \n",
    "                            num_class=4, \n",
    "                            missing=1,\n",
    "                            gamma=0, # default gamma value\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth=3,\n",
    "                            reg_lambda=1, # default L2 value\n",
    "                            subsample=1, # default subsample value\n",
    "                            colsample_bytree=1, # default colsample_bytree value\n",
    "                            early_stopping_rounds=10,\n",
    "                            eval_metric=['merror','mlogloss'],\n",
    "                            seed=42)\n",
    "\n",
    "xgb_clf.fit(X_train, \n",
    "            y_train,\n",
    "            verbose=0, # set to 1 to see xgb training round intermediate results\n",
    "            #sample_weight=sample_weights, # class weights to combat unbalanced 'target'\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "# preparing evaluation metric plots\n",
    "results = xgb_clf.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# xgboost 'mlogloss' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('mlogloss')\n",
    "plt.title('GridSearchCV XGBoost mlogloss')\n",
    "plt.show()\n",
    "\n",
    "# xgboost 'merror' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['merror'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['merror'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('merror')\n",
    "plt.title('GridSearchCV XGBoost merror')\n",
    "plt.show()\n",
    "\n",
    "## ---------- Model Classification Report ----------\n",
    "## get predictions and create model quality report\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('---------------------- XGBoost ----------------------') # unnecessary fancy styling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342ea06",
   "metadata": {},
   "source": [
    "# XGBoost Model v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- XGBoost model v3 ----------\n",
    "## third run of model with \"optimized\" hyperparameters and\n",
    "## attempting to treat unbalanced target classes\n",
    "\n",
    "# balancing 'target' class weights\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y_train)\n",
    "\n",
    "# declaring and fitting xgb classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', \n",
    "                            num_class=4, \n",
    "                            missing=1,\n",
    "                            gamma=0, # default gamma value\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth=5, # re-optimized from v2\n",
    "                            reg_lambda=1, # default L2 value\n",
    "                            #subsample=0.8, # tried but not ideal\n",
    "                            #colsample_bytree=0.3, # tried but not ideal\n",
    "                            early_stopping_rounds=10,\n",
    "                            eval_metric=['merror','mlogloss'],\n",
    "                            seed=42)\n",
    "xgb_clf.fit(X_train, \n",
    "            y_train,\n",
    "            verbose=0, # set to 1 to see xgb training round intermediate results\n",
    "            sample_weight=sample_weights, # class weights to combat unbalanced 'target'\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "# preparing evaluation metric plots\n",
    "results = xgb_clf.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# xgboost 'mlogloss' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('mlogloss')\n",
    "plt.title('GridSearchCV XGBoost mlogloss')\n",
    "plt.show()\n",
    "\n",
    "# xgboost 'merror' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['merror'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['merror'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('merror')\n",
    "plt.title('GridSearchCV XGBoost merror')\n",
    "plt.show()\n",
    "\n",
    "## ---------- Model Classification Report ----------\n",
    "## get predictions and create model quality report\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('---------------------- XGBoost ----------------------') # unnecessary fancy styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83798496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Plot Feature Importance ----------\n",
    "## plotting feature importance of best xgboost model (v3)\n",
    "\n",
    "from xgboost import plot_importance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "plot_importance(xgb_clf, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Plot Model Tree ----------\n",
    "## plotting model tree of best xgboost model (v3)\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 28,12\n",
    "xgb.plot_tree(xgb_clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efc712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc1703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4eceb91",
   "metadata": {},
   "source": [
    "# Old Section: DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc21c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d729e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e41d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Distribution (Positive to Negative): {:.1f}% / {:.1f}%\".format(y_train.mean() * 100, (1 - y_train.mean()) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b597147",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(X.shape[1]))\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=50,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c292cc",
   "metadata": {},
   "source": [
    "# Results : OLD Section: DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ab697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3201dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_test)\n",
    "y_pred = np.squeeze(np.array(model.predict(X_test) >= 0.5, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\\n\", classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b61d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a33601",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
